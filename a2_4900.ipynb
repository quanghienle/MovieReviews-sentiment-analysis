{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a2-4900.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MT9T2kEBWoO",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 2 -- COMP 4900\n",
        "### Student: Tri Cao (100971065) & Hien Le (101044264)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVAxWC3Htb0Y",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# 1. Summary\n",
        "\n",
        "This notebook contains implementations and data analysis details for the second homework, as a detailed reference for our finding. For more compact result please refer to the report.pdf\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qljjf0XL873u",
        "colab_type": "text"
      },
      "source": [
        "# 2. Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TejhUbet-dDN",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9ACVjuP9HGP",
        "colab_type": "text"
      },
      "source": [
        "- Make sure to upload the `train.csv` and `test.csv` data\n",
        "- Import python libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vJENjk59QoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D78UiYkr9z6d",
        "colab_type": "text"
      },
      "source": [
        "- Using `Pandas` to read the _csv_ files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzSsKsMb94F0",
        "colab_type": "code",
        "outputId": "92a3d0df-4064-4295-e574-e7b2babef4e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train_df = pd.read_csv(\"./train.csv\")\n",
        "test_df = pd.read_csv(\"./test.csv\")\n",
        "\n",
        "train_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I should have never watched this movie. The st...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Robert Lansing plays a scientist experimenting...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I was looking forward to this movie. Trustwort...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Where to start? Some guy has some Indian pot t...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What happened? What we have here is basically ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  I should have never watched this movie. The st...  negative\n",
              "1  Robert Lansing plays a scientist experimenting...  negative\n",
              "2  I was looking forward to this movie. Trustwort...  negative\n",
              "3  Where to start? Some guy has some Indian pot t...  negative\n",
              "4  What happened? What we have here is basically ...  negative"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBTzbd4h-QuN",
        "colab_type": "code",
        "outputId": "b99d79b6-aa8a-44a5-9cdb-8cd8135dc62f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "test_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>deals with the issue of Nazism and the vilific...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>/&gt;&lt;br /&gt;What Rambo does not realise is that he...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>I found it charming! Nobody else but Kiarostam...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>I just don't know how this stupid, crap, junk,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Hey guys, &lt;br /&gt;&lt;br /&gt;i have been looking ever...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                             review\n",
              "0   0  deals with the issue of Nazism and the vilific...\n",
              "1   1  /><br />What Rambo does not realise is that he...\n",
              "2   2  I found it charming! Nobody else but Kiarostam...\n",
              "3   3  I just don't know how this stupid, crap, junk,...\n",
              "4   4  Hey guys, <br /><br />i have been looking ever..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQibD6yX-o-C",
        "colab_type": "text"
      },
      "source": [
        "## 2. Prepare data-cleaning models for the pipeline\n",
        "\n",
        "  - There are multiple techniques to analyze and prepare the data to achieve the best results for our models\n",
        "  - Here we are creating a Sklearn's Pipeline to preprocess the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RdfBJ3PBZJf",
        "colab_type": "code",
        "outputId": "5ac576da-8275-4671-a604-2fdf80e64bac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w88gG3NX__ie",
        "colab_type": "text"
      },
      "source": [
        "### 1. Remove non-word characters\n",
        "\n",
        "- This model will remove non-word character from the whole dataset\n",
        "- This includes removing punctuations and numeric characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYFETT1QBdi0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ExtractNonwordCharacters(BaseEstimator):\n",
        "    \"\"\" remove numbers and non-word characters, such as, < > _\n",
        "    \"\"\"\n",
        "    def __init__(self, number=True, non_words=True):\n",
        "        self.number = number\n",
        "        self.non_words = non_words\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, x):\n",
        "        # remove numbers\n",
        "        if self.number:\n",
        "            x = x.str.replace(\"\\d+\", \"\")\n",
        "\n",
        "        # remove non-word characters\n",
        "        if self.non_words:\n",
        "            x = x.str.replace(\"<br />\", \"\", regex=False)\n",
        "            x = x.str.replace(\"[^\\w\\s]\", \"\")\n",
        "            x = x.replace(r\"_\", \"\", regex=True)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiA_0HT1CwP3",
        "colab_type": "code",
        "outputId": "2e95b0ac-dc71-4782-c8a7-0898df3752fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "print(\"Before: \\n\", train_df.review[:1][0])\n",
        "print(\"\\nAfter:\\n\", ExtractNonwordCharacters().transform(train_df.review[:1])[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before: \n",
            " I should have never watched this movie. The style of filming may be considered artsy to some, but it is considered migraine-inducing to me. I think it may have had an interesting plot, but since I couldn't watch it for long stretches at a time I missed a lot. The flickering pictures and stop motion filming branded my brain. I stopped watching mid way through and won't be back for a second try. I suppose if I were home alone in my own lighthouse some dark and stormy evening, this might be just the ticket... PS Not sure if the lighthouse/ film style thing can be considered a spoiler, but I don't want to be blacklisted on my first review ;)\n",
            "\n",
            "After:\n",
            " I should have never watched this movie The style of filming may be considered artsy to some but it is considered migraineinducing to me I think it may have had an interesting plot but since I couldnt watch it for long stretches at a time I missed a lot The flickering pictures and stop motion filming branded my brain I stopped watching mid way through and wont be back for a second try I suppose if I were home alone in my own lighthouse some dark and stormy evening this might be just the ticket PS Not sure if the lighthouse film style thing can be considered a spoiler but I dont want to be blacklisted on my first review \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2abm2qVAFVh",
        "colab_type": "text"
      },
      "source": [
        "### 2. Remove stopwords\n",
        "\n",
        "- Stopwords don't help us to find the context or the true meaning of a sentence.\n",
        "- Thus, they need to to be removed from our datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EFHUIT6DrNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ExtractStopwords(BaseEstimator):\n",
        "    \"\"\" \n",
        "    Remove stopwords from dataset and words that have less than 2 letters\n",
        "    for example, 'the car' --> 'car'\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def remove_stopwords(self, doc):\n",
        "        words_list = [w for w in doc.split() if (w not in self.stop_words) and len(w) > 2]\n",
        "        return \" \".join(words_list)\n",
        "\n",
        "    def transform(self, x):\n",
        "        x = map(self.remove_stopwords, x)\n",
        "        x = np.array(list(x))\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUDSxa-NElVG",
        "colab_type": "code",
        "outputId": "8091f63b-89b3-40ac-9265-671646c7d7cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "print(\"Before: \\n\", train_df.review[:1][0])\n",
        "print(\"\\nAfter:\\n\", ExtractStopwords().transform(train_df.review[:1])[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before: \n",
            " I should have never watched this movie. The style of filming may be considered artsy to some, but it is considered migraine-inducing to me. I think it may have had an interesting plot, but since I couldn't watch it for long stretches at a time I missed a lot. The flickering pictures and stop motion filming branded my brain. I stopped watching mid way through and won't be back for a second try. I suppose if I were home alone in my own lighthouse some dark and stormy evening, this might be just the ticket... PS Not sure if the lighthouse/ film style thing can be considered a spoiler, but I don't want to be blacklisted on my first review ;)\n",
            "\n",
            "After:\n",
            " never watched movie. The style filming may considered artsy some, considered migraine-inducing me. think may interesting plot, since watch long stretches time missed lot. The flickering pictures stop motion filming branded brain. stopped watching mid way back second try. suppose home alone lighthouse dark stormy evening, might ticket... Not sure lighthouse/ film style thing considered spoiler, want blacklisted first review\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7vHkHwPEoVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXC0lnULAMwt",
        "colab_type": "text"
      },
      "source": [
        "### 3. Lemmatize reviews\n",
        "\n",
        "- Since some words mean the same but have different part of speech and in different tense. For example, `am`, `is`, `are`\n",
        "- we need to lemmatize them to reduce the size of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoMW5M5dFn65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lemmatizer(BaseEstimator):\n",
        "    \"\"\" Lemmatize dataset\n",
        "    for example, convert 'is', 'am', 'are' --> 'be'\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "\n",
        "    def get_wordnet_pos(self, word):\n",
        "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "        tag_dict = {\n",
        "            \"J\": wordnet.ADJ,\n",
        "            \"N\": wordnet.NOUN,\n",
        "            \"V\": wordnet.VERB,\n",
        "            \"R\": wordnet.ADV,\n",
        "        }\n",
        "        return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def lemma(self, doc):\n",
        "        words_list = [\n",
        "            self.wnl.lemmatize(w, self.get_wordnet_pos(w)) for w in doc.split()\n",
        "        ]\n",
        "        return \" \".join(words_list)\n",
        "\n",
        "    def transform(self, x):\n",
        "        x = map(self.lemma, x)\n",
        "        x = np.array(list(x))\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrvwSBTUGLJE",
        "colab_type": "code",
        "outputId": "75124034-3be9-4ed0-9b37-20fc0ccc43ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "print(\"Before: \\n\", train_df.review[:1][0])\n",
        "print(\"\\nAfter:\\n\", Lemmatizer().transform(train_df.review[:1])[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before: \n",
            " I should have never watched this movie. The style of filming may be considered artsy to some, but it is considered migraine-inducing to me. I think it may have had an interesting plot, but since I couldn't watch it for long stretches at a time I missed a lot. The flickering pictures and stop motion filming branded my brain. I stopped watching mid way through and won't be back for a second try. I suppose if I were home alone in my own lighthouse some dark and stormy evening, this might be just the ticket... PS Not sure if the lighthouse/ film style thing can be considered a spoiler, but I don't want to be blacklisted on my first review ;)\n",
            "\n",
            "After:\n",
            " I should have never watch this movie. The style of film may be consider artsy to some, but it be consider migraine-inducing to me. I think it may have have an interest plot, but since I couldn't watch it for long stretch at a time I miss a lot. The flicker picture and stop motion film brand my brain. I stop watch mid way through and won't be back for a second try. I suppose if I be home alone in my own lighthouse some dark and stormy evening, this might be just the ticket... PS Not sure if the lighthouse/ film style thing can be consider a spoiler, but I don't want to be blacklist on my first review ;)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPm_tO5ZH3QF",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwqJpoSPAW49",
        "colab_type": "text"
      },
      "source": [
        "### 4. Stem reviews\n",
        "\n",
        "- Similar to lemmatization, Stemming is the process to reduce inflected (or sometimes derived) words to their word stem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGK3a7z8HKAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Stemmer(BaseEstimator):\n",
        "    \"\"\" \n",
        "    stem words by removing some characters at the end of the wood\n",
        "    for example, 'beautiful'  -->  'beauti'\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.stemmer = PorterStemmer()\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def stem(self, doc):\n",
        "        words_list = [\n",
        "            self.stemmer.stem(w) for w in doc.split()\n",
        "        ]\n",
        "        return \" \".join(words_list)\n",
        "\n",
        "    def transform(self, x):\n",
        "        x = map(self.stem, x)\n",
        "        x = np.array(list(x))\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsnRElaqHRc4",
        "colab_type": "code",
        "outputId": "fe55fa4c-fc27-491d-c981-313ceaa2d5b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "print(\"Before: \\n\", train_df.review[:1][0])\n",
        "print(\"\\nAfter:\\n\", Stemmer().transform(train_df.review[:1])[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before: \n",
            " I should have never watched this movie. The style of filming may be considered artsy to some, but it is considered migraine-inducing to me. I think it may have had an interesting plot, but since I couldn't watch it for long stretches at a time I missed a lot. The flickering pictures and stop motion filming branded my brain. I stopped watching mid way through and won't be back for a second try. I suppose if I were home alone in my own lighthouse some dark and stormy evening, this might be just the ticket... PS Not sure if the lighthouse/ film style thing can be considered a spoiler, but I don't want to be blacklisted on my first review ;)\n",
            "\n",
            "After:\n",
            " I should have never watch thi movie. the style of film may be consid artsi to some, but it is consid migraine-induc to me. I think it may have had an interest plot, but sinc I couldn't watch it for long stretch at a time I miss a lot. the flicker pictur and stop motion film brand my brain. I stop watch mid way through and won't be back for a second try. I suppos if I were home alon in my own lighthous some dark and stormi evening, thi might be just the ticket... PS not sure if the lighthouse/ film style thing can be consid a spoiler, but I don't want to be blacklist on my first review ;)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhpFiNI5Hxti",
        "colab_type": "text"
      },
      "source": [
        "### 5. Finally, Run the Pipeline\n",
        "- We have created both stemmed data and lemmztized data, however the result show no difference, thus here we only create a Pipeline for the `Lemmatizer()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNemhw97LqwL",
        "colab_type": "text"
      },
      "source": [
        "- Convert the label (sentiment) to `1` and `0`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYPO4GEwLw7f",
        "colab_type": "code",
        "outputId": "15a51955-5039-4e3f-b6cb-8d01639b1261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "train_df.sentiment = train_df.sentiment.astype(\"category\").cat.codes\n",
        "train_df.sentiment"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        0\n",
              "1        0\n",
              "2        0\n",
              "3        0\n",
              "4        0\n",
              "        ..\n",
              "29995    1\n",
              "29996    1\n",
              "29997    1\n",
              "29998    1\n",
              "29999    1\n",
              "Name: sentiment, Length: 30000, dtype: int8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm9Ktl_eKHu-",
        "colab_type": "text"
      },
      "source": [
        "- Initialize the Pipeline with desired models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQdqzZ23KGRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review_pipeline = Pipeline([(\"Extract-Nonwords\", ExtractNonwordCharacters()),\n",
        "                            (\"Extract-Stopwords\", ExtractStopwords()),\n",
        "                            (\"Lemmatizer\", Lemmatizer())])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHvujHHvK1wt",
        "colab_type": "text"
      },
      "source": [
        "- Fit and transform both train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g919W_hWLCgm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.review = review_pipeline.fit_transform(train_df.review)\n",
        "test_df.review = review_pipeline.transform(test_df.review)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL9DRVopNutE",
        "colab_type": "code",
        "outputId": "beba87df-7a85-45da-c9e6-a64a5773840d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# cleaned train data\n",
        "train_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>never watch movie The style film may consider ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Robert Lansing play scientist experiment passi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>look forward movie Trustworthy actor interest ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Where start Some guy Indian pot he cleaning su...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What happen What basically solid plausible pre...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  never watch movie The style film may consider ...          0\n",
              "1  Robert Lansing play scientist experiment passi...          0\n",
              "2  look forward movie Trustworthy actor interest ...          0\n",
              "3  Where start Some guy Indian pot he cleaning su...          0\n",
              "4  What happen What basically solid plausible pre...          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S49sVHHbNpXz",
        "colab_type": "code",
        "outputId": "35911aad-bf76-447c-d6c6-9273255de33b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# clean test data\n",
        "test_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>deal issue Nazism vilification Germans period ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>What Rambo realise set Trautman portrayed brav...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>found charm Nobody else Kiarostami little yet ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>dont know stupid crap junk garbage good nothin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Hey guy look every find two movie cant find an...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                             review\n",
              "0   0  deal issue Nazism vilification Germans period ...\n",
              "1   1  What Rambo realise set Trautman portrayed brav...\n",
              "2   2  found charm Nobody else Kiarostami little yet ...\n",
              "3   3  dont know stupid crap junk garbage good nothin...\n",
              "4   4  Hey guy look every find two movie cant find an..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9OLamygMlXG",
        "colab_type": "text"
      },
      "source": [
        "# 3. Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AZuZhTfNWcC",
        "colab_type": "text"
      },
      "source": [
        "### 1. Split the train data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAfC8r-ROKcy",
        "colab_type": "code",
        "outputId": "102b56ba-d7a7-4b40-a00b-ec36273ca2ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "\n",
        "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(train_df.review, train_df.sentiment, test_size=0.2, random_state=111)\n",
        "\n",
        "print(\"X_train: \", X_train_df.shape)\n",
        "print(\"X_test : \", X_test_df.shape)\n",
        "print(\"y_train: \", y_train_df.shape)\n",
        "print(\"y_test: \", y_test_df.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train:  (24000,)\n",
            "X_test :  (6000,)\n",
            "y_train:  (24000,)\n",
            "y_test:  (6000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EznAAQVmPAC3",
        "colab_type": "text"
      },
      "source": [
        "### 2. Vectorize the train and test data\n",
        "\n",
        "- Here we taking the top 20,000 words inluding unigram and bigram\n",
        "- Set binaty to `True` and ignores words that occur less than 4 times"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT-fPsIEPIaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vec = CountVectorizer(max_features=20000, ngram_range=(1, 2), binary=True, min_df=4, max_df=0.7)\n",
        "\n",
        "X_train = count_vec.fit_transform(X_train_df)\n",
        "X_test = count_vec.transform(X_test_df)\n",
        "actual_test = count_vec.transform(test_df.review)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_7oz-XHQ2GU",
        "colab_type": "code",
        "outputId": "2c382170-66a3-42f5-d65b-27f5cc710ce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(\"X_train: \", X_train.shape)\n",
        "print(\"X_test: \", X_test.shape)\n",
        "print(\"Actual Test: \", actual_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train:  (24000, 20000)\n",
            "X_test:  (6000, 20000)\n",
            "Actual Test:  (10000, 20000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfVcyAL4RQe4",
        "colab_type": "text"
      },
      "source": [
        "- Convert test sets to numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gktUxWg4RTLS",
        "colab_type": "code",
        "outputId": "72f478a1-cbfc-4fc8-ff02-8edf20d39fc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "y_train = y_train_df.to_numpy()\n",
        "y_test = y_test_df.to_numpy()\n",
        "\n",
        "print(\"y_train: \", y_train.shape)\n",
        "print(\"y_test: \", y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_train:  (24000,)\n",
            "y_test:  (6000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_dThOW6McWY",
        "colab_type": "text"
      },
      "source": [
        "### 3. Data vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDlGaps6Mg04",
        "colab_type": "code",
        "outputId": "afe7fb69-b957-4299-dc33-b4ccdba7287d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "count_vec.vocabulary_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ill': 8273,\n",
              " 'probably': 13432,\n",
              " 'get': 6815,\n",
              " 'lot': 9949,\n",
              " 'hat': 7740,\n",
              " 'movie': 11082,\n",
              " 'guess': 7514,\n",
              " 'didnt': 4043,\n",
              " 'approach': 967,\n",
              " 'proper': 13518,\n",
              " 'nostalgia': 11967,\n",
              " 'generation': 6778,\n",
              " 'but': 2173,\n",
              " 'suffice': 16636,\n",
              " 'say': 14693,\n",
              " 'fire': 6261,\n",
              " 'pretty': 13368,\n",
              " 'even': 5085,\n",
              " 'far': 5579,\n",
              " 'brat': 2000,\n",
              " 'pack': 12581,\n",
              " 'film': 5763,\n",
              " 'yet': 19876,\n",
              " 'another': 829,\n",
              " 'lovely': 10031,\n",
              " 'example': 5293,\n",
              " 'smug': 15817,\n",
              " 'selfindulgent': 15200,\n",
              " 'everything': 5255,\n",
              " 'rent': 14163,\n",
              " 'anyone': 882,\n",
              " 'the': 17106,\n",
              " 'plot': 13088,\n",
              " 'virtually': 18728,\n",
              " 'non': 11935,\n",
              " 'existent': 5355,\n",
              " 'philosophy': 12939,\n",
              " 'best': 1634,\n",
              " 'poorly': 13221,\n",
              " 'deliver': 3872,\n",
              " 'rest': 14247,\n",
              " 'time': 17816,\n",
              " 'complete': 3130,\n",
              " 'lack': 9221,\n",
              " 'anything': 905,\n",
              " 'resemble': 14215,\n",
              " 'sympathetic': 16816,\n",
              " 'character': 2566,\n",
              " 'doesnt': 4307,\n",
              " 'help': 7855,\n",
              " 'situation': 15699,\n",
              " 'there': 17378,\n",
              " 'really': 13852,\n",
              " 'growth': 7504,\n",
              " 'forward': 6506,\n",
              " 'movement': 11081,\n",
              " 'climactic': 2890,\n",
              " 'suicide': 16649,\n",
              " 'scene': 14787,\n",
              " 'refuse': 14065,\n",
              " 'let': 9451,\n",
              " 'death': 3761,\n",
              " 'reality': 13843,\n",
              " 'adult': 336,\n",
              " 'life': 9491,\n",
              " 'enter': 4954,\n",
              " 'each': 4603,\n",
              " 'cookie': 3305,\n",
              " 'cutter': 3634,\n",
              " 'figure': 5758,\n",
              " 'simply': 15635,\n",
              " 'go': 7098,\n",
              " 'make': 10172,\n",
              " 'hate': 7744,\n",
              " 'predictable': 13324,\n",
              " 'way': 19029,\n",
              " 'possible': 13271,\n",
              " 'stalker': 16171,\n",
              " 'creep': 3534,\n",
              " 'discernible': 4198,\n",
              " 'reason': 13950,\n",
              " 'gorgeous': 7318,\n",
              " 'jackass': 8763,\n",
              " 'power': 13296,\n",
              " 'constantly': 3242,\n",
              " 'remind': 14145,\n",
              " 'well': 19125,\n",
              " 'slut': 15784,\n",
              " 'hide': 7905,\n",
              " 'deep': 3823,\n",
              " 'pain': 12587,\n",
              " 'wild': 19420,\n",
              " 'lifestyle': 9539,\n",
              " 'poet': 13153,\n",
              " 'mood': 11009,\n",
              " 'around': 1017,\n",
              " 'full': 6629,\n",
              " 'reveal': 14287,\n",
              " 'actually': 253,\n",
              " 'get lot': 6886,\n",
              " 'hat movie': 7742,\n",
              " 'movie guess': 11228,\n",
              " 'guess didnt': 7516,\n",
              " 'suffice say': 16637,\n",
              " 'yet another': 19878,\n",
              " 'the plot': 17269,\n",
              " 'time the': 17899,\n",
              " 'complete lack': 3131,\n",
              " 'sympathetic character': 16817,\n",
              " 'character doesnt': 2587,\n",
              " 'doesnt help': 4320,\n",
              " 'there really': 17406,\n",
              " 'go make': 7133,\n",
              " 'way possible': 19065,\n",
              " 'possible the': 13272,\n",
              " 'attractive': 1154,\n",
              " 'husband': 8224,\n",
              " 'wife': 19416,\n",
              " 'write': 19788,\n",
              " 'team': 16948,\n",
              " 'robert': 14423,\n",
              " 'wagner': 18778,\n",
              " 'joel': 8858,\n",
              " 'gregory': 7462,\n",
              " 'kate': 8966,\n",
              " 'jackson': 8768,\n",
              " 'donna': 4363,\n",
              " 'arrive': 1035,\n",
              " 'spooky': 16126,\n",
              " 'mansion': 10408,\n",
              " 'actress': 243,\n",
              " 'love': 9983,\n",
              " 'silent': 15608,\n",
              " 'star': 16192,\n",
              " 'harold': 7717,\n",
              " 'house': 8132,\n",
              " 'contract': 3274,\n",
              " 'biography': 1749,\n",
              " 'personal': 12899,\n",
              " 'interest': 8540,\n",
              " 'project': 13496,\n",
              " 'since': 15652,\n",
              " 'father': 5627,\n",
              " 'famed': 5526,\n",
              " 'lover': 10033,\n",
              " 'mysterious': 11645,\n",
              " 'event': 5161,\n",
              " 'unfold': 18414,\n",
              " 'must': 11616,\n",
              " 'fight': 5751,\n",
              " 'save': 14670,\n",
              " 'spirit': 16099,\n",
              " 'beautiful': 1504,\n",
              " 'blonde': 1826,\n",
              " 'perfectly': 12848,\n",
              " 'preserve': 13354,\n",
              " 'crypt': 3585,\n",
              " 'estate': 5057,\n",
              " 'moreover': 11022,\n",
              " 'evil': 5274,\n",
              " 'woman': 19521,\n",
              " 'seem': 15141,\n",
              " 'bent': 1622,\n",
              " 'possess': 13268,\n",
              " 'murder': 11581,\n",
              " 'this': 17612,\n",
              " 'much': 11510,\n",
              " 'night': 11895,\n",
              " 'dark': 3692,\n",
              " 'shadows': 15343,\n",
              " 'variation': 18601,\n",
              " 'genuine': 6792,\n",
              " 'know': 9113,\n",
              " 'play': 13038,\n",
              " 'part': 12647,\n",
              " 'david': 3715,\n",
              " 'intensity': 8532,\n",
              " 'sylvia': 16812,\n",
              " 'sidney': 15593,\n",
              " 'mrs': 11505,\n",
              " 'grayson': 7382,\n",
              " 'hall': 7619,\n",
              " 'hill': 7940,\n",
              " 'match': 10528,\n",
              " 'lara': 9254,\n",
              " 'parker': 12641,\n",
              " 'diana': 4031,\n",
              " 'bill': 1735,\n",
              " 'macy': 10108,\n",
              " 'oscar': 12476,\n",
              " 'good': 7188,\n",
              " 'would': 19668,\n",
              " 'played': 13057,\n",
              " 'john': 8862,\n",
              " 'dan': 3664,\n",
              " 'curtis': 3618,\n",
              " 'smooth': 15814,\n",
              " 'cameo': 2264,\n",
              " 'joan': 8842,\n",
              " 'blondell': 1827,\n",
              " 'carradine': 2405,\n",
              " 'dorothy': 4427,\n",
              " 'delivery': 3877,\n",
              " 'resembles': 14216,\n",
              " 'bennett': 1620,\n",
              " 'begs': 1561,\n",
              " 'question': 13639,\n",
              " 'producer': 13466,\n",
              " 'aaron': 0,\n",
              " 'original': 12455,\n",
              " 'regular': 14076,\n",
              " 'director': 4142,\n",
              " 'bridget': 2037,\n",
              " 'work': 19590,\n",
              " 'here': 7883,\n",
              " 'come': 2995,\n",
              " 'arguably': 999,\n",
              " 'storyline': 16483,\n",
              " 'differs': 4099,\n",
              " 'angle': 796,\n",
              " 'eternal': 5067,\n",
              " 'end': 4815,\n",
              " 'closely': 2907,\n",
              " 'laura': 9335,\n",
              " 'phoenix': 12940,\n",
              " 'husband wife': 8225,\n",
              " 'silent film': 15610,\n",
              " 'film star': 6089,\n",
              " 'silent movie': 15611,\n",
              " 'movie star': 11404,\n",
              " 'this much': 17646,\n",
              " 'play part': 13051,\n",
              " 'part well': 12664,\n",
              " 'good part': 7259,\n",
              " 'joan blondell': 8843,\n",
              " 'john carradine': 8865,\n",
              " 'youve': 19975,\n",
              " 'see': 14998,\n",
              " 'romero': 14494,\n",
              " 'yes': 19871,\n",
              " 'and': 731,\n",
              " 'ladder': 9230,\n",
              " 'right': 14358,\n",
              " 'later': 9300,\n",
              " 'hellraiser': 7852,\n",
              " 'okay': 12136,\n",
              " 'three': 17742,\n",
              " 'jam': 8777,\n",
              " 'together': 17966,\n",
              " 'whole': 19366,\n",
              " 'big': 1701,\n",
              " 'mess': 10710,\n",
              " 'sound': 15991,\n",
              " 'like': 9555,\n",
              " 'terrible': 17032,\n",
              " 'absolutely': 34,\n",
              " 'godawful': 7167,\n",
              " 'yeah': 19838,\n",
              " 'indie': 8409,\n",
              " 'flick': 6378,\n",
              " 'give': 7007,\n",
              " 'crap': 3494,\n",
              " 'pas': 12683,\n",
              " 'filmic': 6177,\n",
              " 'excrement': 5334,\n",
              " 'attempt': 1132,\n",
              " 'establish': 5054,\n",
              " 'credibility': 3526,\n",
              " 'focus': 6407,\n",
              " 'interaction': 8538,\n",
              " 'evident': 5272,\n",
              " 'unfortunately': 18419,\n",
              " 'writer': 19806,\n",
              " 'theyre': 17464,\n",
              " 'isnt': 8647,\n",
              " 'living': 9800,\n",
              " 'dead': 3743,\n",
              " 'shout': 15475,\n",
              " 'inane': 8361,\n",
              " 'line': 9714,\n",
              " 'vain': 18574,\n",
              " 'caught': 2482,\n",
              " 'microphone': 10750,\n",
              " 'set': 15286,\n",
              " 'dialogue': 4026,\n",
              " 'never': 11775,\n",
              " 'for': 6442,\n",
              " 'youd': 19912,\n",
              " 'think': 17532,\n",
              " 'something': 15903,\n",
              " 'what': 19283,\n",
              " 'are': 988,\n",
              " 'going': 7174,\n",
              " 'dont': 4365,\n",
              " 'weve': 19278,\n",
              " 'got': 7326,\n",
              " 'leave': 9398,\n",
              " 'lets': 9468,\n",
              " 'just': 8944,\n",
              " 'accept': 69,\n",
              " 'our': 12497,\n",
              " 'fate': 5625,\n",
              " 'then': 17370,\n",
              " 'maybe': 10586,\n",
              " 'exaggeration': 5290,\n",
              " 'exchange': 5326,\n",
              " 'reach': 13787,\n",
              " 'level': 9474,\n",
              " 'bad': 1292,\n",
              " 'thing': 17472,\n",
              " 'half': 7604,\n",
              " 'dream': 4483,\n",
              " 'zero': 19988,\n",
              " 'purpose': 13595,\n",
              " 'nothing': 11998,\n",
              " 'relevance': 14111,\n",
              " 'youve see': 19979,\n",
              " 'movie yes': 11482,\n",
              " 'right and': 14361,\n",
              " 'movie okay': 11318,\n",
              " 'let make': 9458,\n",
              " 'make movie': 10265,\n",
              " 'movie three': 11433,\n",
              " 'together make': 17969,\n",
              " 'make whole': 10319,\n",
              " 'sound like': 15999,\n",
              " 'like good': 9604,\n",
              " 'movie terrible': 11419,\n",
              " 'the film': 17180,\n",
              " 'film attempt': 5789,\n",
              " 'character interaction': 2603,\n",
              " 'theyre good': 17466,\n",
              " 'good character': 7199,\n",
              " 'this isnt': 17637,\n",
              " 'night living': 11897,\n",
              " 'living dead': 9801,\n",
              " 'set the': 15300,\n",
              " 'the dialogue': 17158,\n",
              " 'for movie': 6449,\n",
              " 'movie focus': 11211,\n",
              " 'much character': 11517,\n",
              " 'youd think': 19914,\n",
              " 'think character': 17540,\n",
              " 'character would': 2660,\n",
              " 'would something': 19744,\n",
              " 'something say': 15928,\n",
              " 'say what': 14759,\n",
              " 'dont know': 4385,\n",
              " 'know well': 9184,\n",
              " 'something well': 15936,\n",
              " 'well go': 19168,\n",
              " 'weve get': 19279,\n",
              " 'bad thing': 1361,\n",
              " 'thing movie': 17507,\n",
              " 'movie half': 11230,\n",
              " 'rest movie': 14251,\n",
              " 'movie the': 11422,\n",
              " 'popular': 13234,\n",
              " 'screen': 14911,\n",
              " 'british': 2062,\n",
              " 'picture': 12963,\n",
              " 'century': 2513,\n",
              " 'theatre': 17356,\n",
              " 'craft': 3488,\n",
              " 'solid': 15871,\n",
              " 'drama': 4464,\n",
              " 'ever': 5169,\n",
              " 'develop': 4003,\n",
              " 'ongoing': 12385,\n",
              " 'twist': 18267,\n",
              " 'lie': 9488,\n",
              " 'pile': 12986,\n",
              " 'act': 115,\n",
              " 'flawless': 6364,\n",
              " 'cast': 2437,\n",
              " 'marshal': 10486,\n",
              " 'first': 6266,\n",
              " 'julian': 8924,\n",
              " 'outstanding': 12523,\n",
              " 'performance': 12852,\n",
              " 'usual': 18557,\n",
              " 'tom': 17982,\n",
              " 'wilkinson': 19427,\n",
              " 'turn': 18238,\n",
              " 'concerned': 3171,\n",
              " 'include': 8375,\n",
              " 'support': 16695,\n",
              " 'actor': 196,\n",
              " 'linda': 9710,\n",
              " 'audience': 1159,\n",
              " 'engross': 4901,\n",
              " 'couple': 3451,\n",
              " 'shock': 15431,\n",
              " 'incident': 8371,\n",
              " 'jump': 8931,\n",
              " 'tight': 17804,\n",
              " 'production': 13470,\n",
              " 'minute': 10857,\n",
              " 'produce': 13462,\n",
              " 'limited': 9706,\n",
              " 'budget': 2113,\n",
              " 'success': 16619,\n",
              " 'direct': 4125,\n",
              " 'some': 15880,\n",
              " 'publicity': 13564,\n",
              " 'suggest': 16642,\n",
              " 'per': 12835,\n",
              " 'nigel': 11894,\n",
              " 'obviously': 12096,\n",
              " 'case': 2424,\n",
              " 'recommended': 14011,\n",
              " 'view': 18681,\n",
              " 'well craft': 19145,\n",
              " 'cast well': 2459,\n",
              " 'first time': 6315,\n",
              " 'time director': 17833,\n",
              " 'outstanding performance': 12524,\n",
              " 'support actor': 16696,\n",
              " 'film include': 5940,\n",
              " 'really make': 13900,\n",
              " 'make jump': 10240,\n",
              " 'around minute': 1024,\n",
              " 'limited budget': 9707,\n",
              " 'big screen': 1720,\n",
              " 'film seem': 6068,\n",
              " 'almost': 501,\n",
              " 'certainly': 2520,\n",
              " 'stag': 16158,\n",
              " 'montage': 10998,\n",
              " 'reading': 13802,\n",
              " 'briefly': 2040,\n",
              " 'naked': 11659,\n",
              " 'front': 6612,\n",
              " 'anonymous': 828,\n",
              " 'uncredited': 18356,\n",
              " 'panel': 12615,\n",
              " 'charles': 2672,\n",
              " 'band': 1403,\n",
              " 'thrown': 17780,\n",
              " 'random': 13730,\n",
              " 'surrender': 16756,\n",
              " 'many': 10410,\n",
              " 'primarily': 13407,\n",
              " 'lesbian': 9441,\n",
              " 'most': 11038,\n",
              " 'rid': 14343,\n",
              " 'horse': 8107,\n",
              " 'hmmm': 7992,\n",
              " 'polish': 13192,\n",
              " 'despite': 3976,\n",
              " 'audition': 1175,\n",
              " 'secret': 14986,\n",
              " 'camera': 2266,\n",
              " 'behind': 1569,\n",
              " 'still': 16300,\n",
              " 'introduces': 8594,\n",
              " 'talk': 16910,\n",
              " 'sexy': 15335,\n",
              " 'enjoy': 4905,\n",
              " 'one': 12180,\n",
              " 'taylor': 16943,\n",
              " 'look': 9868,\n",
              " 'weird': 19121,\n",
              " 'excuse': 5338,\n",
              " 'aside': 1068,\n",
              " 'obvious': 12095,\n",
              " 'draw': 4473,\n",
              " 'frankly': 6550,\n",
              " 'dull': 4547,\n",
              " 'uninspiring': 18431,\n",
              " 'experience': 5380,\n",
              " 'almost certainly': 504,\n",
              " 'reading line': 13805,\n",
              " 'get naked': 6897,\n",
              " 'many movie': 10430,\n",
              " 'woman like': 19526,\n",
              " 'make still': 10299,\n",
              " 'movie scene': 11374,\n",
              " 'scene one': 14826,\n",
              " 'probably best': 13434,\n",
              " 'excuse movie': 5340,\n",
              " 'naked woman': 11660,\n",
              " 'great': 7384,\n",
              " 'eye': 5439,\n",
              " 'style': 16577,\n",
              " 'his': 7958,\n",
              " 'inyourface': 8623,\n",
              " 'asset': 1089,\n",
              " 'away': 1229,\n",
              " 'also': 546,\n",
              " 'particularly': 12675,\n",
              " 'building': 2126,\n",
              " 'thick': 17469,\n",
              " 'layer': 9350,\n",
              " 'dread': 4480,\n",
              " 'atmosphere': 1118,\n",
              " 'standout': 16184,\n",
              " 'shot': 15463,\n",
              " 'introduction': 8595,\n",
              " 'killer': 9065,\n",
              " 'slowly': 15776,\n",
              " 'emerges': 4784,\n",
              " 'trap': 18103,\n",
              " 'usually': 18558,\n",
              " 'admit': 327,\n",
              " 'minority': 10855,\n",
              " 'shut': 15575,\n",
              " 'firmly': 6265,\n",
              " 'gotten': 7332,\n",
              " 'wrong': 19813,\n",
              " 'sure': 16717,\n",
              " 'people': 12766,\n",
              " 'hard': 7683,\n",
              " 'consider': 3224,\n",
              " 'recent': 13982,\n",
              " 'genre': 6785,\n",
              " 'entry': 4993,\n",
              " 'afraid': 373,\n",
              " 'horror': 8097,\n",
              " 'paris': 12637,\n",
              " 'hilton': 7945,\n",
              " 'could': 3353,\n",
              " 'possibly': 13273,\n",
              " 'worth': 19650,\n",
              " 'watch': 18927,\n",
              " 'who': 19357,\n",
              " 'stylish': 16581,\n",
              " 'effective': 4687,\n",
              " 'lifelong': 9538,\n",
              " 'fan': 5546,\n",
              " 'thoroughly': 17681,\n",
              " 'wax': 19028,\n",
              " 'lurid': 10085,\n",
              " 'sadistic': 14595,\n",
              " 'glory': 7091,\n",
              " 'safely': 14601,\n",
              " 'accomplishment': 87,\n",
              " 'modern': 10943,\n",
              " 'along': 529,\n",
              " 'sin': 15648,\n",
              " 'city': 2828,\n",
              " 'completely': 3134,\n",
              " 'film success': 6099,\n",
              " 'just think': 8949,\n",
              " 'turn away': 18240,\n",
              " 'particularly good': 12677,\n",
              " 'like would': 9692,\n",
              " 'sure people': 16726,\n",
              " 'film consider': 5832,\n",
              " 'much well': 11564,\n",
              " 'horror flick': 8102,\n",
              " 'paris hilton': 12638,\n",
              " 'could possibly': 3395,\n",
              " 'worth watch': 19664,\n",
              " 'who know': 19360,\n",
              " 'horror film': 8101,\n",
              " 'horror fan': 8100,\n",
              " 'thoroughly enjoy': 17682,\n",
              " 'house wax': 8137,\n",
              " 'sin city': 15649,\n",
              " 'coen': 2946,\n",
              " 'brothers': 2085,\n",
              " 'truly': 18168,\n",
              " 'wonderful': 19549,\n",
              " 'saga': 14604,\n",
              " 'escape': 5034,\n",
              " 'convict': 3296,\n",
              " 'though': 17685,\n",
              " 'base': 1437,\n",
              " 'odyssey': 12117,\n",
              " 'ancient': 730,\n",
              " 'homer': 8036,\n",
              " 'read': 13791,\n",
              " 'able': 10,\n",
              " 'follow': 6415,\n",
              " 'story': 16381,\n",
              " 'brother': 2083,\n",
              " 'woven': 19773,\n",
              " 'celluloid': 2499,\n",
              " 'delight': 3866,\n",
              " 'soundtrack': 16006,\n",
              " 'indeed': 8396,\n",
              " 'product': 13469,\n",
              " 'wrap': 19775,\n",
              " 'paper': 12619,\n",
              " 'wonderfully': 19559,\n",
              " 'exploit': 5397,\n",
              " 'back': 1258,\n",
              " 'day': 3722,\n",
              " 'old': 12137,\n",
              " 'rich': 14330,\n",
              " 'whose': 19394,\n",
              " 'appearance': 955,\n",
              " 'add': 299,\n",
              " 'texture': 17057,\n",
              " 'authenticity': 1193,\n",
              " 'george': 6796,\n",
              " 'clooney': 2903,\n",
              " 'magnificent': 10132,\n",
              " 'grease': 7383,\n",
              " 'haired': 7599,\n",
              " 'everett': 5202,\n",
              " 'honest': 8044,\n",
              " 'con': 3160,\n",
              " 'run': 14552,\n",
              " 'pompous': 13203,\n",
              " 'vocabulary': 18748,\n",
              " 'comical': 3070,\n",
              " 'endear': 4872,\n",
              " 'where': 19330,\n",
              " 'art': 1040,\n",
              " 'easily': 4627,\n",
              " 'date': 3710,\n",
              " 'effort': 4692,\n",
              " 'enough': 4928,\n",
              " 'warrant': 18893,\n",
              " 'nomination': 11933,\n",
              " 'tim': 17812,\n",
              " 'blake': 1797,\n",
              " 'portrayal': 13253,\n",
              " 'dimwitted': 4117,\n",
              " 'friend': 6587,\n",
              " 'deserve': 3955,\n",
              " 'nod': 11923,\n",
              " 'read the': 13799,\n",
              " 'follow story': 6421,\n",
              " 'story the': 16462,\n",
              " 'the soundtrack': 17311,\n",
              " 'film indeed': 5941,\n",
              " 'back day': 1259,\n",
              " 'old film': 12145,\n",
              " 'character actor': 2568,\n",
              " 'film add': 5771,\n",
              " 'george clooney': 6797,\n",
              " 'film date': 5846,\n",
              " 'best effort': 1644,\n",
              " 'good enough': 7215,\n",
              " 'best actor': 1637,\n",
              " 'best film': 1648,\n",
              " 'having': 7764,\n",
              " 'listen': 9741,\n",
              " 'harvey': 7733,\n",
              " 'omen': 12174,\n",
              " 'commentary': 3086,\n",
              " 'discover': 4205,\n",
              " 'absolute': 32,\n",
              " 'piece': 12970,\n",
              " 'rubbish': 14537,\n",
              " 'its': 8687,\n",
              " 'might': 10771,\n",
              " 'glimpse': 7082,\n",
              " 'middle': 10752,\n",
              " 'remote': 14150,\n",
              " 'bit': 1759,\n",
              " 'michael': 10736,\n",
              " 'confront': 3196,\n",
              " 'describe': 3949,\n",
              " 'high': 7910,\n",
              " 'school': 14865,\n",
              " 'les': 9440,\n",
              " 'mouth': 11067,\n",
              " 'hung': 8208,\n",
              " 'open': 12395,\n",
              " 'disbelief': 4194,\n",
              " 'laugh': 9311,\n",
              " 'dire': 4124,\n",
              " 'minimum': 10847,\n",
              " 'ten': 17008,\n",
              " 'point': 13158,\n",
              " 'clever': 2878,\n",
              " 'its like': 8711,\n",
              " 'like really': 9656,\n",
              " 'really bad': 13859,\n",
              " 'bad movie': 1339,\n",
              " 'movie might': 11296,\n",
              " 'high school': 7918,\n",
              " 'laugh much': 9322,\n",
              " 'know write': 9190,\n",
              " 'ten line': 17009,\n",
              " 'line make': 9720,\n",
              " 'make point': 10275,\n",
              " 'there nothing': 17401,\n",
              " 'you': 19890,\n",
              " 'commercial': 3088,\n",
              " 'guy': 7549,\n",
              " 'whatever': 19303,\n",
              " 'he': 7777,\n",
              " 'try': 18182,\n",
              " 'keep': 8980,\n",
              " 'spit': 16102,\n",
              " 'sink': 15681,\n",
              " 'thats': 17076,\n",
              " 'metaphor': 10721,\n",
              " 'kept': 9009,\n",
              " 'difficult': 4100,\n",
              " 'fresh': 6578,\n",
              " 'feel': 5674,\n",
              " 'left': 9408,\n",
              " 'taste': 16931,\n",
              " 'premise': 13340,\n",
              " 'corny': 3330,\n",
              " 'fun': 6638,\n",
              " 'thousand': 17733,\n",
              " 'year': 19840,\n",
              " 'return': 14280,\n",
              " 'romania': 14485,\n",
              " 'priest': 13405,\n",
              " 'knew': 9103,\n",
              " 'kill': 9051,\n",
              " 'long': 9842,\n",
              " 'agent': 396,\n",
              " 'bottom': 1945,\n",
              " 'amok': 706,\n",
              " 'lose': 9941,\n",
              " 'dialog': 4025,\n",
              " 'less': 9444,\n",
              " 'enthusiastic': 4973,\n",
              " 'human': 8179,\n",
              " 'lead': 9352,\n",
              " 'do': 4279,\n",
              " 'poor': 13210,\n",
              " 'man': 10345,\n",
              " 'van': 18590,\n",
              " 'helsing': 7869,\n",
              " 'check': 2699,\n",
              " 'brain': 1985,\n",
              " 'door': 4420,\n",
              " 'dreary': 4489,\n",
              " 'monday': 10969,\n",
              " 'you know': 19898,\n",
              " 'he try': 7790,\n",
              " 'try really': 18212,\n",
              " 'really hard': 13890,\n",
              " 'hard keep': 7693,\n",
              " 'movie kept': 11265,\n",
              " 'kept watch': 9012,\n",
              " 'watch even': 18948,\n",
              " 'even though': 5148,\n",
              " 'though really': 17704,\n",
              " 'this movie': 17645,\n",
              " 'movie left': 11274,\n",
              " 'bad taste': 1358,\n",
              " 'taste mouth': 16932,\n",
              " 'for first': 6447,\n",
              " 'time like': 17859,\n",
              " 'thousand year': 17736,\n",
              " 'run amok': 14553,\n",
              " 'completely lose': 3137,\n",
              " 'act part': 146,\n",
              " 'lead the': 9362,\n",
              " 'the best': 17122,\n",
              " 'best act': 1635,\n",
              " 'end movie': 4844,\n",
              " 'movie feel': 11204,\n",
              " 'feel like': 5683,\n",
              " 'like poor': 9652,\n",
              " 'poor man': 13216,\n",
              " 'van helsing': 18593,\n",
              " 'might get': 10781,\n",
              " 'give star': 7054,\n",
              " 'they': 17441,\n",
              " 'boy': 1968,\n",
              " 'fantastically': 5575,\n",
              " 'bbc': 1487,\n",
              " 'finally': 6206,\n",
              " 'brought': 2086,\n",
              " 'doctor': 4295,\n",
              " 'saturday': 14665,\n",
              " 'belongs': 1607,\n",
              " 'episode': 5002,\n",
              " 'rose': 14511,\n",
              " 'strength': 16516,\n",
              " 'new': 11827,\n",
              " 'series': 15255,\n",
              " 'davies': 3718,\n",
              " 'executive': 5344,\n",
              " 'hes': 7896,\n",
              " 'show': 15477,\n",
              " 'want': 18822,\n",
              " 'experienced': 5383,\n",
              " 'billie': 1738,\n",
              " 'piper': 12995,\n",
              " 'tyler': 18315,\n",
              " 'doctors': 4297,\n",
              " 'companion': 3104,\n",
              " 'credible': 3527,\n",
              " 'strong': 16531,\n",
              " 'nice': 11867,\n",
              " 'classic': 2849,\n",
              " 'two': 18271,\n",
              " 'world': 19624,\n",
              " 'start': 16212,\n",
              " 'double': 4431,\n",
              " 'special': 16047,\n",
              " 'effect': 4676,\n",
              " 'technology': 16961,\n",
              " 'create': 3514,\n",
              " 'real': 13809,\n",
              " 'believable': 1580,\n",
              " 'environment': 4994,\n",
              " 'monster': 10992,\n",
              " 'rubber': 14536,\n",
              " 'suit': 16650,\n",
              " 'sort': 15981,\n",
              " 'hint': 7951,\n",
              " 'regard': 14067,\n",
              " 'past': 12694,\n",
              " 'intrigue': 8590,\n",
              " 'and boy': 737,\n",
              " 'episode one': 5003,\n",
              " 'executive producer': 5345,\n",
              " 'fan show': 5561,\n",
              " 'fan want': 5563,\n",
              " 'know people': 9162,\n",
              " 'people want': 12827,\n",
              " 'show first': 15500,\n",
              " 'first episode': 6272,\n",
              " 'back story': 1273,\n",
              " 'strong character': 16533,\n",
              " 'character also': 2570,\n",
              " 'also nice': 592,\n",
              " 'nice see': 11874,\n",
              " 'two the': 18305,\n",
              " 'the end': 17168,\n",
              " 'end the': 4859,\n",
              " 'the world': 17348,\n",
              " 'end story': 4857,\n",
              " 'special effect': 16049,\n",
              " 'evening': 5160,\n",
              " 'room': 14503,\n",
              " 'offering': 12126,\n",
              " 'diane': 4032,\n",
              " 'keaton': 8977,\n",
              " 'meryl': 10706,\n",
              " 'streep': 16512,\n",
              " 'every': 5203,\n",
              " 'same': 14622,\n",
              " 'vanessa': 18595,\n",
              " 'redgrave': 14037,\n",
              " 'patrick': 12712,\n",
              " 'wilson': 19443,\n",
              " 'glenn': 7079,\n",
              " 'close': 2904,\n",
              " 'least': 9376,\n",
              " 'instead': 8501,\n",
              " 'found': 6514,\n",
              " 'sometimes': 15941,\n",
              " 'cannot': 2299,\n",
              " 'overcome': 12540,\n",
              " 'trite': 18146,\n",
              " 'simplistic': 15634,\n",
              " 'occasion': 12097,\n",
              " 'offensive': 12123,\n",
              " 'problem': 13450,\n",
              " 'structure': 16540,\n",
              " 'cut': 3626,\n",
              " 'forth': 6499,\n",
              " 'tell': 16981,\n",
              " 'era': 5019,\n",
              " 'verse': 18635,\n",
              " 'recall': 13977,\n",
              " 'felt': 5710,\n",
              " 'mistake': 10912,\n",
              " 'rivet': 14408,\n",
              " 'clarify': 2843,\n",
              " 'matter': 10539,\n",
              " 'fact': 5462,\n",
              " 'meaningless': 10623,\n",
              " 'girl': 6984,\n",
              " 'meet': 10642,\n",
              " 'loses': 9946,\n",
              " 'fashion': 5612,\n",
              " 'unbelievable': 18342,\n",
              " 'clichd': 2884,\n",
              " 'spoiler': 16113,\n",
              " 'bear': 1493,\n",
              " 'mind': 10828,\n",
              " 'continue': 3267,\n",
              " 'claire': 2838,\n",
              " 'danes': 3674,\n",
              " 'brutally': 2102,\n",
              " 'miscast': 10881,\n",
              " 'not': 11969,\n",
              " 'go see': 7144,\n",
              " 'reason movie': 13961,\n",
              " 'meryl streep': 10707,\n",
              " 'love every': 9993,\n",
              " 'every minute': 5218,\n",
              " 'vanessa redgrave': 18596,\n",
              " 'would least': 19717,\n",
              " 'least good': 9381,\n",
              " 'sometimes even': 15942,\n",
              " 'even great': 5112,\n",
              " 'great actor': 7387,\n",
              " 'one occasion': 12302,\n",
              " 'way film': 19044,\n",
              " 'actually enjoy': 259,\n",
              " 'enjoy movie': 4909,\n",
              " 'back forth': 1264,\n",
              " 'long one': 9854,\n",
              " 'actually show': 282,\n",
              " 'matter fact': 10541,\n",
              " 'fact make': 5468,\n",
              " 'make seem': 10284,\n",
              " 'boy girl': 1970,\n",
              " 'girl get': 6986,\n",
              " 'claire danes': 2839,\n",
              " 'arthur': 1047,\n",
              " 'willie': 19438,\n",
              " 'similar': 15620,\n",
              " 'theme': 17364,\n",
              " 'grave': 7377,\n",
              " 'rob': 14417,\n",
              " 'anthology': 855,\n",
              " 'consistent': 3236,\n",
              " 'mythology': 11653,\n",
              " 'true': 18160,\n",
              " 'happen': 7656,\n",
              " 'simultaneously': 15647,\n",
              " 'contradict': 3275,\n",
              " 'saw': 14684,\n",
              " 'trick': 18135,\n",
              " 'treat': 18119,\n",
              " 'avoids': 1217,\n",
              " 'sell': 15202,\n",
              " 'fall': 5513,\n",
              " 'design': 3963,\n",
              " 'intend': 8529,\n",
              " 'united': 18444,\n",
              " 'recognize': 13996,\n",
              " 'insane': 8475,\n",
              " 'nitpick': 11914,\n",
              " 'fast': 5615,\n",
              " 'loose': 9927,\n",
              " 'history': 7972,\n",
              " 'stretch': 16518,\n",
              " 'scottish': 14900,\n",
              " 'maiden': 10138,\n",
              " 'precursor': 13318,\n",
              " 'french': 6568,\n",
              " 'abandon': 1,\n",
              " 'hundred': 8204,\n",
              " 'burke': 2147,\n",
              " 'hare': 7713,\n",
              " 'mention': 10687,\n",
              " 'execute': 5342,\n",
              " 'like horror': 9610,\n",
              " 'didnt help': 4054,\n",
              " 'saw film': 14685,\n",
              " 'the dead': 17153,\n",
              " 'problem film': 13451,\n",
              " 'film play': 6027,\n",
              " 'might able': 10772,\n",
              " 'film set': 6071,\n",
              " 'hundred year': 8207,\n",
              " 'year later': 19849,\n",
              " 'floor': 6391,\n",
              " 'ive': 8742,\n",
              " 'huge': 8168,\n",
              " 'scratch': 14906,\n",
              " 'hooked': 8059,\n",
              " 'list': 9740,\n",
              " 'documentary': 4300,\n",
              " 'cant': 2309,\n",
              " 'remember': 14135,\n",
              " 'last': 9266,\n",
              " 'energy': 4883,\n",
              " 'refresh': 14058,\n",
              " 'insightful': 8486,\n",
              " 'preachy': 13311,\n",
              " 'value': 18583,\n",
              " 'cool': 3307,\n",
              " 'amaze': 664,\n",
              " 'overall': 12531,\n",
              " 'smart': 15800,\n",
              " 'entertain': 4957,\n",
              " 'enlighten': 4922,\n",
              " 'ive never': 8749,\n",
              " 'huge fan': 8171,\n",
              " 'never really': 11810,\n",
              " 'really felt': 13878,\n",
              " 'felt like': 5712,\n",
              " 'like one': 9644,\n",
              " 'one cant': 12203,\n",
              " 'cant remember': 2326,\n",
              " 'last time': 9285,\n",
              " 'time much': 17873,\n",
              " 'much fun': 11530,\n",
              " 'fun watch': 6644,\n",
              " 'watch documentary': 18940,\n",
              " 'documentary style': 4302,\n",
              " 'production value': 13477,\n",
              " 'shot film': 15464,\n",
              " 'till': 17808,\n",
              " 'hbo': 7776,\n",
              " 'begin': 1551,\n",
              " 'rerun': 14210,\n",
              " 'month': 11002,\n",
              " 'loud': 9975,\n",
              " 'theater': 17355,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-6u4Fxav1W5",
        "colab_type": "text"
      },
      "source": [
        "#  4. Naive Bayes Classifier Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSM-7YtIR3EF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyNaiveBayes(BaseEstimator, ClassifierMixin):\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        theta_y = sum(y) / len(y)\n",
        "        sum_X = np.array([np.array(np.sum(X[y == k], axis=0)).flatten() for k in range(2)]).T\n",
        "        sum_y = np.array([len(y) - sum(y), sum(y)])\n",
        "        self.theta_X = (sum_X + 1) / (sum_y + 2)\n",
        "        self.log_1 = np.log(theta_y / (1 - theta_y))\n",
        "        self.log_2 = np.log(self.theta_X[:, 1] / self.theta_X[:, 0])\n",
        "        self.log_3 = np.log((1 - self.theta_X[:, 1]) / (1 - self.theta_X[:, 0]))\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_arr = X.toarray()\n",
        "        self.logits = np.full(X.shape[0], self.log_1) + X @ self.log_2 + (1 - X_arr) @ self.log_3\n",
        "        return (self.logits > 0).astype(int)\n",
        "\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        pred = np.random.rand(X.shape[0], self.classes_.size)\n",
        "        return pred / np.sum(pred, axis=1)[:, np.newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAVNPXSdSG4Q",
        "colab_type": "text"
      },
      "source": [
        "# 5. Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyizkcD1TZKg",
        "colab_type": "text"
      },
      "source": [
        "- The function is implemented in order to make it eaiser for testing different  classification models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "115ofGrcSZOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import time\n",
        "\n",
        "def test_model(clf, train_data, test_data):\n",
        "    X_train, y_train = train_data\n",
        "    X_test, y_test = test_data\n",
        "    scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
        "    print(f\"Training score: {scores}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    runtime = time.time() - start_time\n",
        "\n",
        "    target_names = [\"positive\", \"negative\"]\n",
        "    reports = classification_report(y_test, y_pred, target_names=target_names)\n",
        "    print(reports)\n",
        "    return y_pred, runtime\n",
        "\n",
        "def calculate_accuracy(labels, pred):\n",
        "  correct = (labels == pred)\n",
        "  return correct.sum() / correct.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d_NDcYAGQ37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# store the results\n",
        "test_results = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvo3x0seT3Z1",
        "colab_type": "text"
      },
      "source": [
        "### 1. Test with our Naive Bayes Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4lZidd-Si-f",
        "colab_type": "code",
        "outputId": "5da7fd34-1121-454a-ceb4-8f0e41c65041",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "train_data = [X_train, y_train]\n",
        "test_data = [X_test, y_test]\n",
        "\n",
        "nb = MyNaiveBayes()\n",
        "\n",
        "nb_y_pred, nb_runtime = test_model(nb, train_data, test_data)\n",
        "\n",
        "nb_accu = calculate_accuracy(y_test, nb_y_pred)\n",
        "test_results.append({'accuracy': nb_accu, 'runtime': nb_runtime})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training score: [0.84645833 0.85625    0.85041667 0.84520833 0.84770833]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.87      0.83      0.85      2991\n",
            "    negative       0.84      0.87      0.86      3009\n",
            "\n",
            "    accuracy                           0.85      6000\n",
            "   macro avg       0.85      0.85      0.85      6000\n",
            "weighted avg       0.85      0.85      0.85      6000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K98tNGeRUaxX",
        "colab_type": "text"
      },
      "source": [
        "### 2. Test with Sklearn's LinearSVC model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plOW9taXUfVG",
        "colab_type": "code",
        "outputId": "6e690fa9-8af9-4914-af74-cb972ea8dd45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svc = LinearSVC(max_iter=3000, C=0.05)\n",
        "\n",
        "svc_y_pred, svc_runtime = test_model(svc, train_data, test_data)\n",
        "\n",
        "svc_accu = calculate_accuracy(y_test, svc_y_pred)\n",
        "test_results.append({'accuracy': svc_accu, 'runtime': svc_runtime})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training score: [0.84       0.85520833 0.84770833 0.84833333 0.84458333]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.86      0.84      0.85      2991\n",
            "    negative       0.85      0.87      0.86      3009\n",
            "\n",
            "    accuracy                           0.85      6000\n",
            "   macro avg       0.85      0.85      0.85      6000\n",
            "weighted avg       0.85      0.85      0.85      6000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvJaEpRjT9qF",
        "colab_type": "text"
      },
      "source": [
        "### 2. Test with Sklearn's Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE68pj8aUDZC",
        "colab_type": "code",
        "outputId": "816e6c43-b84c-4fa3-bf2f-2988389efa3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "lr = LogisticRegression(max_iter=3000, C=0.05)\n",
        "\n",
        "lr_y_pred, lr_runtime = test_model(lr, train_data, test_data)\n",
        "\n",
        "lr_accu = calculate_accuracy(y_test, lr_y_pred)\n",
        "test_results.append({'accuracy': lr_accu, 'runtime': lr_runtime})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training score: [0.85395833 0.86583333 0.86125    0.86270833 0.85958333]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.88      0.85      0.86      2991\n",
            "    negative       0.86      0.88      0.87      3009\n",
            "\n",
            "    accuracy                           0.87      6000\n",
            "   macro avg       0.87      0.87      0.87      6000\n",
            "weighted avg       0.87      0.87      0.87      6000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4r83az6wL_w",
        "colab_type": "text"
      },
      "source": [
        "# 5. Results and conclusions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyuZlLviYhL4",
        "colab_type": "text"
      },
      "source": [
        "### 1. Accuracy of Classification models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Xcv6EruKvCb",
        "colab_type": "code",
        "outputId": "e0a2a69c-d785-4e53-92be-9fc8dff0cef5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_axis_labels = ['Naive Bayes', 'LinearSVC', 'Logistic Regresion']\n",
        "y_axis = [t['accuracy'] for t in test_results]\n",
        "plt.bar([1,2,3], y_axis, color=\"blue\")\n",
        "plt.xlabel(\"Training models\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy comparison\")\n",
        "plt.xticks([1, 2, 3], x_axis_labels)\n",
        "plt.savefig('accu.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAdeUlEQVR4nO3dfbylc73/8dd7ZtyFIWYUMxgxyiDK\njgpRVDiHURHz02GiqBORVOrnOPOTUyTpzjmPEJL7dE5NUkMynYiyx70RBo0Zt9vNYORu+Pz++H6X\nuVrW3nuZ2dfas/f3/Xw81mNfN9/rur7rWmuv93V9r3V9lyICMzMr14jBroCZmQ0uB4GZWeEcBGZm\nhXMQmJkVzkFgZlY4B4GZWeEcBGYFkbSfpMsHux62bJHvI7A6SJoJbAG8OSJeGOTqmFkffEZgA07S\nBGB7IIA9OrztUZ3c3lDifWO9cRBYHfYHrgPOBg6ozpC0kqSTJc2V9JSkqyWtlOdtJ+lPkhZImidp\nap4+U9KnKuuYKunqynhI+pyku4G787Tv5XU8LWmWpO0r5UdK+pqkeyQ9k+evK+lUSSc31Xe6pC+0\nepKSNpV0haQnJD0i6Wt5+gqSvivpwfz4rqQV8rwdJc2X9GVJj0p6SNKeknaTdFde19cq25gm6RJJ\nF+W63iBpi8r8oyvPY7akjzTtp2sknSLpcWBadd8pOSXX42lJt0raLM9bTdI5knrya3WMpBHV/S/p\n25KelHSfpF37fVfYsisi/PBjQB/AHOBfga2Al4A3VeadCswExgEjgfcCKwDrA88AU4DlgDWBLfMy\nM4FPVdYxFbi6Mh7AFcAawEp52ifyOkYBXwQeBlbM874E3Aq8FRCpCWtNYGvgQWBELjcG+Hu1/pVt\nrgo8lNe9Yh7fJs87jhSEawFjgT8BX8/zdgQWAcfm5/lpoAc4P69jU+A5YINcflreh3vl8kcB9wHL\n5fl7A+uQDur2AZ4F1q7sp0XAYXk/rFTdd8CHgVnA6nk/bFJZ9hzgl7lOE4C7gIMq630p130k8Nm8\n3zTY7z0/lvB/drAr4MfwegDb5Q+JMXn8r8AX8vCI/CG3RYvlvgr8Ty/rbCcIPtBPvZ5sbBe4E5jc\nS7k7gA/m4UOBy3opNwW4sZd59wC7VcY/DPwtD++Y98HIPL5qrv82lfKzgD3z8DTgusq8EaQA2r6X\nbd/UeG55P93fNL8aBB/IH/DvJodfnj4SeBGYVJl2CDCzso45lXlvyM/hzYP9/vNjyR5uGrKBdgBw\neUQ8lsfPZ3Hz0BjS0fM9LZZbt5fp7ZpXHZF0lKQ7cvPTAmC1vP3+tvUT0tkE+e9PeynX1zrWAeZW\nxufmaQ2PR8TLefi5/PeRyvzngFUq468+t4h4BZjfWJ+k/SXdlJvTFgCbsfh5/sOyzSLi98APSWdp\nj0o6TdLovPxyLZ7DuMr4w5X1/D0PVutsQ4iDwAZMbuv/OLCDpIclPQx8Adgit2s/BjwPbNhi8Xm9\nTIfU3PGGyvibW5R59etv+XrAl3Nd3hgRqwNPkZo/+tvWucDkXN9NgF/0Um4e8JZe5j1IaupqWC9P\nW1LrNgZyO/144EFJ6wOnk85c1szP8zYWP0+o7JdWIuL7EbEVMAnYmNRs9hjprK75OTywFM/BlmEO\nAhtIewIvkz5UtsyPTYA/Avvno9kzge9IWidftH1PvpB6HrCzpI9LGiVpTUlb5vXeBHxU0hskbQQc\n1E89ViW1jfcAoyQdC4yuzD8D+LqkifmC6dslrQkQEfOB60lnAj+PiOdo7VJgbUlH5IvDq0raJs+7\nADhG0lhJY0jXA87tf/f1aitJH1X61s8RwAukaxArkz7oewAkfZJ0RtAWSe+StI2k5Uhh+zzwSj5b\nuRj4j/y81geOXMrnYMswB4ENpAOAsyLi/oh4uPEgNT/slz/IjiJdqL0eeAI4kdQ+fT+wG+ni6xOk\nD//Gt2NOIbVZP0Jqujmvn3rMAH5Lav+eS/qAqzaRfIf0QXc58DTwY9KF1IafAJvTe7MQEfEM8EFg\nd1Izyd3A+/Ps44Fu4Jb8XG/I05bUL0kXgp8E/gX4aES8FBGzgZOBa0n7ZnPgmtex3tGkM4onSfvp\nceCkPO8wUjjcC1xNauI7cymegy3DfEOZWRNJ7yMd/a4fg/wPImkasFFEfKK/smZLymcEZhW5meRw\n4IzBDgGzTnEQmGWSNgEWAGsD3x3k6ph1jJuGzMwK5zMCM7PCDblOqMaMGRMTJkwY7GqYmQ0ps2bN\neiwixraaN+SCYMKECXR3dw92NczMhhRJc3ub56YhM7PCOQjMzArnIDAzK5yDwMyscA4CM7PCOQjM\nzArnIDAzK5yDwMyscA4CM7PCDbk7i81s2Sb1X8aWTF19hPqMwMyscA4CM7PCFdU05FPW+tR1yurX\nrD7+KRJr8BmBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWuFqDQNIuku6U\nNEfS0S3mryfpKkk3SrpF0m511sfMzF6rtiCQNBI4FdgVmARMkTSpqdgxwMUR8Q5gX+A/66qPmZm1\nVucZwdbAnIi4NyJeBC4EJjeVCWB0Hl4NeLDG+piZWQt19jU0DphXGZ8PbNNUZhpwuaTDgJWBnWus\nj5mZtTDYF4unAGdHxHhgN+Cnkl5TJ0kHS+qW1N3T09PxSpqZDWd1BsEDwLqV8fF5WtVBwMUAEXEt\nsCIwpnlFEXFaRHRFRNfYsWNrqq6ZWZnqDILrgYmSNpC0POli8PSmMvcDOwFI2oQUBD7kNzProNqC\nICIWAYcCM4A7SN8Oul3ScZL2yMW+CHxa0s3ABcDUCPeSbmbWSbX+ME1EXAZc1jTt2MrwbGDbOutg\nZmZ9G+yLxWZmNsgcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZm\nhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZ\nWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFg\nZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVrtYgkLSLpDslzZF0dC9lPi5p\ntqTbJZ1fZ33MzOy1RtW1YkkjgVOBDwLzgeslTY+I2ZUyE4GvAttGxJOS1qqrPmZm1lqdZwRbA3Mi\n4t6IeBG4EJjcVObTwKkR8SRARDxaY33MzKyFOoNgHDCvMj4/T6vaGNhY0jWSrpO0S6sVSTpYUrek\n7p6enpqqa2ZWpsG+WDwKmAjsCEwBTpe0enOhiDgtIroiomvs2LEdrqKZ2fBWZxA8AKxbGR+fp1XN\nB6ZHxEsRcR9wFykYzMysQ+oMguuBiZI2kLQ8sC8wvanML0hnA0gaQ2oqurfGOpmZWZPagiAiFgGH\nAjOAO4CLI+J2ScdJ2iMXmwE8Lmk2cBXwpYh4vK46mZnZaykiBrsOr0tXV1d0d3cv0bLSAFfGXlXX\n28ivWX38mg09S/OaSZoVEV2t5g32xWIzMxtkDgIzs8I5CMzMCucgMDMrnIPAzKxwDgIzs8I5CMzM\nCucgMDMrnIPAzKxw/QaBpMMkvbETlTEzs85r54zgTaRfF7s4//SkbyA3MxtG+g2CiDiG1DX0j4Gp\nwN2SviFpw5rrZmZmHdDWNYJIPdM9nB+LgDcCl0j6Vo11MzOzDuj3x+slHQ7sDzwGnEHqKvolSSOA\nu4Ev11tFMzOrU79BAKwBfDQi5lYnRsQrkv65nmqZmVmntNM09BvgicaIpNGStgGIiDvqqpiZmXVG\nO0HwX8DCyvjCPM3MzIaBdoJAUfkZs4h4hfaalMzMbAhoJwjulfR5Scvlx+H4B+bNzIaNdoLgM8B7\ngQeA+cA2wMF1VsrMzDqn3yaeiHgU2LcDdTEzs0HQzn0EKwIHAZsCKzamR8SBNdbLzMw6pJ2moZ8C\nbwY+DPwBGA88U2elzMysc9oJgo0i4t+AZyPiJ8A/ka4TmJnZMNBOELyU/y6QtBmwGrBWfVUyM7NO\naud+gNPy7xEcA0wHVgH+rdZamZlZx/QZBLljuacj4kngf4G3dKRWZmbWMX02DeW7iN27qJnZMNbO\nNYLfSTpK0rqS1mg8aq+ZmZl1RDvXCPbJfz9XmRa4mcjMbFho587iDTpRETMzGxzt3Fm8f6vpEXHO\nwFfHzMw6rZ2moXdVhlcEdgJuABwEZmbDQDtNQ4dVxyWtDlxYW43MzKyj2vnWULNnAV83MDMbJtq5\nRvAr0reEIAXHJODiOitlZmad0841gm9XhhcBcyNifk31MTOzDmsnCO4HHoqI5wEkrSRpQkT8rdaa\nmZlZR7RzjeBnwCuV8ZfzNDMzGwbaCYJREfFiYyQPL9/OyiXtIulOSXMkHd1HuY9JCkld7azXzMwG\nTjtB0CNpj8aIpMnAY/0tJGkkcCqwK+kC8xRJk1qUWxU4HPhzu5U2M7OB004QfAb4mqT7Jd0PfAU4\npI3ltgbmRMS9+SziQmByi3JfB04Enm+zzmZmNoD6DYKIuCci3k06qp8UEe+NiDltrHscMK8yPj9P\ne5WkdwLrRsSv+1qRpIMldUvq7unpaWPTZmbWrn6DQNI3JK0eEQsjYqGkN0o6fmk3nH/05jvAF/sr\nGxGnRURXRHSNHTt2aTdtZmYV7TQN7RoRCxoj+dfKdmtjuQeAdSvj4/O0hlWBzYCZkv4GvBuY7gvG\nZmad1U4QjJS0QmNE0krACn2Ub7gemChpA0nLA/uSfvMYgIh4KiLGRMSEiJgAXAfsERHdr+sZmJnZ\nUmnnhrLzgCslnQUImAr8pL+FImKRpEOBGcBI4MyIuF3ScUB3REzvew1mZtYJ7fQ+eqKkm4GdSX0O\nzQDWb2flEXEZcFnTtGN7KbtjO+s0M7OB1W7vo4+QQmBv4APAHbXVyMzMOqrXMwJJGwNT8uMx4CJA\nEfH+DtXNzMw6oK+mob8CfwT+uXHfgKQvdKRWZmbWMX01DX0UeAi4StLpknYiXSw2M7NhpNcgiIhf\nRMS+wNuAq4AjgLUk/ZekD3WqgmZmVq92uph4NiLOj4jdSTeF3Ujqb8jMzIaB1/WbxRHxZO7uYae6\nKmRmZp21JD9eb2Zmw4iDwMyscA4CM7PCOQjMzArnIDAzK5yDwMyscA4CM7PCOQjMzArnIDAzK5yD\nwMyscA4CM7PCOQjMzArnIDAzK5yDwMyscA4CM7PCOQjMzArnIDAzK5yDwMyscA4CM7PCOQjMzArn\nIDAzK5yDwMyscA4CM7PCOQjMzArnIDAzK5yDwMyscA4CM7PCOQjMzArnIDAzK5yDwMyscA4CM7PC\nOQjMzApXaxBI2kXSnZLmSDq6xfwjJc2WdIukKyWtX2d9zMzstWoLAkkjgVOBXYFJwBRJk5qK3Qh0\nRcTbgUuAb9VVHzMza63OM4KtgTkRcW9EvAhcCEyuFoiIqyLi73n0OmB8jfUxM7MW6gyCccC8yvj8\nPK03BwG/aTVD0sGSuiV19/T0DGAVzcxsmbhYLOkTQBdwUqv5EXFaRHRFRNfYsWM7Wzkzs2FuVI3r\nfgBYtzI+Pk/7B5J2Bv4vsENEvFBjfczMrIU6zwiuByZK2kDS8sC+wPRqAUnvAH4E7BERj9ZYFzMz\n60VtQRARi4BDgRnAHcDFEXG7pOMk7ZGLnQSsAvxM0k2SpveyOjMzq0mdTUNExGXAZU3Tjq0M71zn\n9s3MrH/LxMViMzMbPA4CM7PCOQjMzArnIDAzK5yDwMyscA4CM7PCOQjMzArnIDAzK5yDwMyscA4C\nM7PCOQjMzArnIDAzK5yDwMyscA4CM7PCOQjMzArnIDAzK5yDwMyscA4CM7PCOQjMzArnIDAzK5yD\nwMyscA4CM7PCOQjMzArnIDAzK5yDwMyscA4CM7PCOQjMzArnIDAzK5yDwMyscA4CM7PCOQjMzArn\nIDAzK5yDwMyscA4CM7PCOQjMzArnIDAzK5yDwMyscA4CM7PCOQjMzApXaxBI2kXSnZLmSDq6xfwV\nJF2U5/9Z0oQ662NmZq9VWxBIGgmcCuwKTAKmSJrUVOwg4MmI2Ag4BTixrvqYmVlrdZ4RbA3MiYh7\nI+JF4EJgclOZycBP8vAlwE6SVGOdzMysyaga1z0OmFcZnw9s01uZiFgk6SlgTeCxaiFJBwMH59GF\nku6spcbLnjE07YtlleMbGEKvF/g1y0p6zdbvbUadQTBgIuI04LTBrkenSeqOiK7Broe1x6/X0OPX\nLKmzaegBYN3K+Pg8rWUZSaOA1YDHa6yTmZk1qTMIrgcmStpA0vLAvsD0pjLTgQPy8F7A7yMiaqyT\nmZk1qa1pKLf5HwrMAEYCZ0bE7ZKOA7ojYjrwY+CnkuYAT5DCwhYrrjlsiPPrNfT4NQPkA3Azs7L5\nzmIzs8I5CMzMCucgaIOkkHRyZfwoSdP6WWaPVt1qLMG2p0rqkXSTpNslXSLpDUu73hJJWthi2mck\n7d+BbR8o6VZJt0i6TdJkSQdIuqCp3Jj8eq8gaTlJJ0i6W9INkq6VtGvddV1SrfbvEqxjHUmX9DF/\ndUn/2m75FsufLem+/P90s6SdlrbOS0rSGS16WxgUvkbQBknPAw8B74qIxyQdBawSEdM6sO2pQFdE\nHJrHzweuiIiz6t72cCNpYUSs0uFtivQV6auAd0bEU5JWAcaSvip9L7BeRPw9l/8MsHVEHCjpBGBt\n4OCIeEHSm4AdIuLiTj6HdnVi/+b+yC6NiM2WcPmz8/KXSHo/cFpETByAeo2KiEVLu57B4jOC9iwi\nfbvgC80zJO2eO8y7UdLv8j9r40j+h5JWkzRX0og8fWVJ8/LR3oaSfitplqQ/SnpbX5XI91qsDDzZ\n27YljchHkGNzmRG5U7+x+fFzSdfnx7a5zA75COmmvK5VB3LnLcskTcvBjqSZkk6U9BdJd0naPk8f\nKemkvM9ukXRInr6KpCvz0fqtkibn6ROUOls8B7gN2AB4BlgIEBELI+K+iHga+AOwe6VK+wIX5LO+\nTwOHRcQLeblHltUQ6E3eF7/P++1KSevl6RtKui7vt+MbZxO5/G15eNP8WtyUl58InABsmKed1FR+\npKRv5zOuWyQd1k/1riX1btCo61aS/pD/H2dIWjtPf1deX2Obje1NlTRd0u+BK/O0L1XeJ/8vT1tZ\n0q/zGchtkvbJ02dK6srDU/K+uE3SiZU6LZT0H3nZ6xqfLwMuIvzo50H6Bx4N/I1009tRwLQ8740s\nPrP6FHByHp4K/DAP/xJ4fx7eBzgjD18JTMzD25Duo2je9lSgB7gJeAT4IzCyn23/O3BEHv4Q8PM8\nfD6wXR5eD7gjD/8K2DYPrwKMGux9Xtfr2GLaNOCoPDyzsg93A36Xhw8GjsnDKwDdpA/3UcDoPH0M\nMAcQMAF4BXh3njeS9DXq+4GzgN0r298L+J88vA7wYC7/duDGwd5nA7B/fwUckIcPBH6Rhy8FpuTh\nzzSWzfvutjz8A2C/PLw8sFJ1fovynyX1WTYqj6/Roj5nA3vl4T2B8/PwcsCfgLF5fB/SV94hhfl7\n8vAJle1NJXWds0Ye/xDpgFGkg+xLgfcBHwNOr9Rhtcr7rSu/7veTzhJHAb8H9sxlovF+Ab7VeB8O\n9MNnBG2KdPR2DvD5plnjgRmSbgW+BGzaYvGLSG8sSEd8Fyk1D7wX+Jmkm4AfkZoBWrkoIrYE3gw0\nttPXts8EGu3eB5I+fAB2Bn6YtzcdGJ3rcQ3wHUmfB1aPIXyKOwD+O/+dRfqQgfQPvn/eb38m9Yc1\nkfQP/w1JtwC/Ix1dNo7Y5kbEdQAR8TKwC+lD/y7gFC2+xvRrYFtJo4GPk0L75dqeXee9h3QAAvBT\nYLvK9J/l4fObF8quBb4m6SvA+hHxXD/b2hn4UeP9GxFP9FLuJEl35e02jr7fCmwGXJFf52OA8ZJW\nB1aNiGt7qesVle18KD9uBG4A3kZ6n9wKfDCfbW4fEU81reNdwMyI6Ml1P48UIAAvkgIF/vE9OaAc\nBK/Pd0ldZ69cmfYD0pH/5sAhwIotlpsO7CJpDWArUuKPABZExJaVxyZ9bTzSYcGvWPwmabntiJgH\nPCLpA6ReYH+Ty48gHaU2tjcuUjPFCaQzipWAa/prohrmXsh/X2bxDZciNdE09tsGEXE5sB/pKG6r\nHNSPsPj1f7a60kj+EhHfJB0MfCxPfw74LfCRPL1x8XgOsF4OiCJFxPnAHsBzwGX5/TwQvhQRGwNf\nIR00QXqNb6+8xptHxIfaWFf1dRbwzco6NoqIH0fEXcA7SYFwvKRjX0ddX8r/9/CP78kB5SB4HXLy\nX0wKg4bVWNyH0gGvWSgtt5DU5cb3SBeqXs5nGPdJ2hvSRUVJW7RRje2Ae9rY9hnAucDPKkeYlwOv\ntptK2jL/3TAibo2IE3M9Sw6CVmYAn5W0HICkjSWtTNr/j0bES0oXHlv27qj0zZZ3ViZtCcytjF8A\nHEk6m7gWINLF4x8D31PqogWlazx7D+xTq92fWNxjwH6kpk2A68hhSC89Ckh6C3BvRHyf1Lz6dtK1\nlt6uYV0BHKJ0LY184NWXHwIjJH0YuBMYK+k9ednlJG0aEQuAZyQ1ek7uq/eDGcCB+SwbSeMkrSVp\nHeDvEXEucBIpFKr+Auyg9I2xkcAU0rWjjnEQvH4nk9qDG6aRmndm0Xd3thcBn8h/G/YDDpJ0M3A7\nr/29hoZ9GhfMgHcAX29j29NJ7f3Vbxd9HujKF7Jmk9pmAY5oXGADXmLxGcRw8wZJ8yuPI9tc7gxg\nNnBDvlD4I9KR2Xmk/XkrqSnur70svxzwbUl/zc0O+wCHV+ZfQWonvqhy9AepeaIHmJ23eynwdJt1\nHgyt9u9hwCfze+tfWPy8jwCOzNM3ApqbSyA1ld2W99lmwDkR8TjprPU2SSc1lT+D1NZ+S/6f+j99\nVTbv6+OBL0f6zZS9gBPzsjeRmm4hHfidnuuxci91JZ8lng9cm98Tl5BCa3PgL3n5f8/brC73EHA0\n6ZtlNwOzIuKXfdV9oPnro8NU/jbCKRGx/WDXxayZ0reinouIkLQv6cJxbwdCg0rSKvmsHqV7g9aO\niMP7WWxIGRK/R2CvT36zfpZ0xmG2LNqK9MUFAQtIX2pYVv2TpK+SPi/nkr4tNKz4jMDMrHC+RmBm\nVjgHgZlZ4RwEZmaFcxDYkCFpTS3uE+lhSQ9Uxpdvcx1nSXprP2U+J2mZu9Au6VxJey5tGbNm/taQ\nDRn5O+SNm+Cmkfqn+Xa1TP4WiiLilV7W8ck2tnPq0tfWbOjwGYENeZI2kjRb0nmkG/PWlnSapG6l\n33A4tlL2aklbSholaYFSf/83K/X1v1Yuc7ykIyrlT1DqBfNOSe/N01dW6sl1ttJvRHQ37tRuqtt8\nSd/I27he0jslXS7pHkmfzmVGSPpOvknqVkl7Vab/Z74R7QoqNzIq9Yj5B6WeMn+jFr1SKvWUOTvf\nQHhi83yzBgeBDRdvI91ANykiHgCOjoguYAtSh1+tfgBkNeAPEbEFqWuH3r7LrojYmtSxXyNUDgMe\njohJpDu939FH3e7L27iO1G3ER0h3rTbuEN8b2KRRV1KndGuR7nTdAJgEfDIvg6QVSN2VfCwitiJ1\nJfJ1KnIw7AZsGhFvB77ZR/2scG4asuHinojoroxPkXQQ6T2+DunDdHbTMs9FRKM7jVlAb3dht+qR\ndDtyz5URcbOk2/uo2/T891ZSF8nPAs9KekWpX5rtgAtyn1APS7qa1D3x+/L0V4D5kmbm9WxC6mn2\nd6kljJGk7pCrniB1hX26pF+zuAdLs9dwENhw8WovkEo/YHI46Ze+Fkg6l9a9wr5YGe6rZ8dWPZK+\nHo3lX6kMN8aXZH0Cbumr+5DcEV4X6Qxjb9Kd5u30pmkFctOQDUejSb1UPq30K1MfrmEb15A6RUPS\n5qQzjiX1R2DffE3gTcC2pB+/+V9Sh4MjJI0DdsjlZwPjJG2dt7+8pH/4HQylX5kbHRGXkn5Zr6+m\nKyuczwhsOLqB9GH5V1LfMNfUsI0fAOco9eLaeLTslbINlwDvBm4h/SLVkRHxqNKPsr8/r/t+FndR\n/UK+oPx9pd8rGEnqFbfaPLUa8N/5esIIUjfXZi25ryGzJaDU5/2oiHg+N0VdTvrZ0ZJ/3c2GKJ8R\nmC2ZVYArcyAIOMQhYEOVzwjMzArni8VmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoX7/7TyBgn6zIxe\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuTLuh8FYnyw",
        "colab_type": "text"
      },
      "source": [
        "### 2. Runtime of Classification models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHKOobF-YTiL",
        "colab_type": "code",
        "outputId": "516e5f53-f8a8-46b2-f0bb-2981d1c5f987",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "y_runtime = [t['runtime'] for t in test_results]\n",
        "plt.bar([1,2,3], y_runtime, color=\"blue\")\n",
        "plt.xlabel(\"Training models\")\n",
        "plt.ylabel(\"Runtime\")\n",
        "plt.title(\"Runtime comparison\")\n",
        "plt.xticks([1, 2, 3], x_axis_labels)\n",
        "plt.savefig('runtime.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAdAUlEQVR4nO3deZxcVZ3+8c+ThIASlmHSKCSEIEQl\nKGuDICgoisBvTJifiGRkMIAgzoA6KAqOIoOyDS4jgkpERfZtRg0QDTsCEiCsWVgmoJhEgQCyr4Hv\n/HFOmUtR3V1J+nan+zzv16tefevWufd+a+l67npKEYGZmZVrSH8XYGZm/ctBYGZWOAeBmVnhHARm\nZoVzEJiZFc5BYGZWOAeBDQqSfizp6/1dx4rKr491R76OwOog6Y/AW4BXgWeB3wKHRMSzvTDvycCn\nI2KH5Z2XmXmLwOr10YgYAWwObAEc2c/1FEnS0P6uwVZsDgKrXUQ8DEwnBQIAkq6V9OnK/cmSbqjc\nD0kHS/pfSU9KOlXJxsCPge0kPSvpydz+DEnfysM7SVog6cuSHpX0F0l7SNpd0v2SnpD01cqyhkg6\nQtIDkh6XdKGktbp6PpImSrpT0tN5ml3z+HUlTc3znyfpwMo0R0u6SNLZkp6RNEvS2yUdmWucL2mX\nptfneEm35OX8ulpTntfDkp6S9DtJm1QeO0PSjyRNk/Qc8IGm12ekpEvz6/qEpOslDcmPbZyX/aSk\nOZImNM33VEmX5edws6QN2/wY2ArMQWC1kzQa2A2Yt5ST/gOwNbApsBfwkYi4BzgYuCkiRkTEml1M\n+1ZgFWAUcBTwE2AfYCvgfcDXJW2Q2x4K7AHsCKwL/BU4tYvnsg1wJnA4sCbwfuCP+eHzgQV5HnsC\nx0n6YGXyjwJnAX8H3EEKxyG5xmOA05oWty+wP7AOsBg4ufLYb4BxwNrA7cA5TdP+E3AssBpwQ9Nj\nX8x1dpB2330VCEkrAZcAl+f5HgqcI+kdlWn3Bv4jP4d5eRk2wDkIrE6/kvQMMB94FPjGUk5/QkQ8\nGRF/Aq6hskXRhleAYyPiFdIX9Ejg+xHxTETMAeYCm+W2BwP/HhELIuIl4GhgT0nDWsz3AOBnEXFF\nRLwWEQsj4l5J6wHbA1+JiBcj4k7gdNKXecP1ETE9IhYDF5G+iE+o1DhWUjXYzoqI2RHxHPB1YK/G\nbp6I+Fl+Lo16N5O0RmXaX0fEjbnGF1u8NusA60fEKxFxfaSDhdsCI3JNL0fE1cClwKTKtL+MiFvy\ncziHpXtPbAXlILA67RERqwE7Ae8kfRkvjYcrw8+TvqTa9XhEvJqHX8h/H6k8/kJlfusDv8y7Q54E\n7iEd5H5Li/muBzzQYvy6wBMR8Uxl3EOktf2G5uU/1qLG6nOc3zSvlYCRkoZKOiHvlnqaJVskI7uY\nttlJpLX5yyU9KOmIynOYHxGvdfMcluc9sRWUg8BqFxHXAWcA366Mfg54c+X+W5dmlr1QVtV8YLeI\nWLNyWyUiFnbRttV+8T8Da0larTJuDNBqHu1ar2lerwCPkXb7TAQ+BKwBjM1tVGnf5WuUtyS+GBFv\nAyYAh0naOT+H9RrHC3rpOdgA4CCwvvJfwIclNXbH3An8f0lvlrQRaZdLux4BRksa3ku1/Rg4VtL6\nAJI6JE3sou1Pgf0k7ZwPMo+S9M6ImA/8Hjhe0iqSNiU9p7OXo659JI2X9GbSMYSL8xbEasBLwOOk\nMD1uaWYq6R8kbSRJwFOkrZ/XgJtJa/lflrSSpJ1IxzXOX47nYAOAg8D6REQsIh1kPSqP+h7wMulL\n/Re88WBnd64G5gAPS3qsF8r7PjCVtKvkGWAG8J5WDSPiFmA/Uv1PAdeRdi1B2pc+lrRm/UvgGxFx\n5XLUdRZpS+ph0oHvz+XxZ5J22SwkHeuYsZTzHQdcSbq+4ybghxFxTUS8TPri34205fFDYN+IuHc5\nnoMNAL6gzGwFJOla4OyIOL2/a7HBz1sEZmaFcxCYmRXOu4bMzApX2xaBpJ/lS+dnd/G4JJ2cL8W/\nW9KWddViZmZda3XlZG85AziFdIZDK7uRzl4YRzpD40d0caZG1ciRI2Ps2LG9U6GZWSFuu+22xyKi\no9VjtQVBRPxO0thumkwEzsyXts+QtKakdSLiL93Nd+zYscycObMXKzUzG/wkPdTVY/15sHgUr78M\nfgGvv5T9byQdJGmmpJmLFi3qk+LMzEoxIM4aiogpEdEZEZ0dHS23bMzMbBn1ZxAs5PV9qYzGfZqY\nmfW5/gyCqcC++eyhbYGnejo+YGZmva+2g8WSziN1PzxS0gJSX/QrAUTEj4FpwO6k7nCfJ/XfYmZm\nfazOs4Ym9fB4AP9a1/LNzKw9A+JgsZmZ1cdBYGZWOAeBmVnh6uxiYoUj9dzGlo37LjQbuLxFYGZW\nOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZ\nFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARm\nZoUb1t8FmNngIvV3BYNXRD3z9RaBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoWrNQgk7Srp\nPknzJB3R4vExkq6RdIekuyXtXmc9Zmb2RrUFgaShwKnAbsB4YJKk8U3NvgZcGBFbAHsDP6yrHjMz\na63OLYJtgHkR8WBEvAycD0xsahPA6nl4DeDPNdZjZmYt1BkEo4D5lfsL8riqo4F9JC0ApgGHtpqR\npIMkzZQ0c9GiRXXUamZWrP4+WDwJOCMiRgO7A2dJekNNETElIjojorOjo6PPizQzG8zqDIKFwHqV\n+6PzuKoDgAsBIuImYBVgZI01mZlZkzqD4FZgnKQNJA0nHQye2tTmT8DOAJI2JgWB9/2YmfWh2oIg\nIhYDhwDTgXtIZwfNkXSMpAm52ReBAyXdBZwHTI6oq389MzNrpdZuqCNiGukgcHXcUZXhucD2ddZg\nZmbd6++DxWZm1s8cBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZm\nhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZ\nWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVblh/F2DWHam/Kxi8Ivq7AltReIvAzKxw\nDgIzs8I5CMzMCucgMDMrnIPAzKxwDgIzs8LVGgSSdpV0n6R5ko7oos1ekuZKmiPp3DrrMTOzN6rt\nOgJJQ4FTgQ8DC4BbJU2NiLmVNuOAI4HtI+Kvktauqx4zM2utzi2CbYB5EfFgRLwMnA9MbGpzIHBq\nRPwVICIerbEeMzNroc4gGAXMr9xfkMdVvR14u6QbJc2QtGurGUk6SNJMSTMXLVpUU7lmZmXq74PF\nw4BxwE7AJOAnktZsbhQRUyKiMyI6Ozo6+rhEM7PBrc4gWAisV7k/Oo+rWgBMjYhXIuIPwP2kYDAz\nsz7SVhAo2UfSUfn+GEnb9DDZrcA4SRtIGg7sDUxtavMr0tYAkkaSdhU9uBT1m5nZcmp3i+CHwHak\n3TcAz5DOCOpSRCwGDgGmA/cAF0bEHEnHSJqQm00HHpc0F7gGODwiHl/K52BmZsuh3dNH3xMRW0q6\nAyCf6jm8p4kiYhowrWncUZXhAA7LNzMz6wftbhG8kq8LCABJHcBrtVVlZmZ9pt0gOBn4JbC2pGOB\nG4DjaqvKzMz6TFu7hiLiHEm3ATsDAvaIiHtqrczMzPrE0nQx8QhwfZ7mTZK2jIjb6ynLzMz6SltB\nIOmbwGTgAfJxgvz3g/WUZWZmfaXdLYK9gA1zn0FmZjaItHuweDbwhq4fzMxs4Gt3i+B44A5Js4GX\nGiMjYkLXk5iZ2UDQbhD8AjgRmIWvHzAzG1TaDYLnI+LkWisxM7N+0W4QXC/peFKncdVdQz591Mxs\ngGs3CLbIf7etjPPpo2Zmg0C7VxZ/oO5CzMysf3QbBJL2iYizJbXsHTQivltPWWZm1ld62iJYNf9d\nrcVj0WKcmZkNMN0GQUSclgevjIgbq49J2r62qszMrM+0e2XxD9ocZ2ZmA0xPxwi2A94LdDQdJ1gd\nGFpnYWZm1jd6OkYwHBiR21WPEzwN7FlXUWZm1nd6OkZwHXCdpDMi4qE+qsnMzPpQuxeUrSxpCjC2\nOk1E+IIyM7MBrt0guAj4MXA68Gp95ZiZWV9rNwgWR8SPaq3EzMz6Rbunj14i6V8krSNprcat1srM\nzKxPtLtF8Kn89/DKuADe1rvlmJlZX2u307kN6i7EzMz6R1tBIGnfVuMj4szeLcfMzPpau7uGtq4M\nrwLsDNwOOAjMzAa4dncNHVq9L2lN4PxaKjIzsz7V7llDzZ4DfNzAzGwQaPcYwSUs+f2BIcB40kVm\nZmY2wLV7jODbleHFwEMRsaCGeszMrI+1e4zguup9SUMkfTIizqmnLDMz6yvdHiOQtLqkIyWdImkX\nJYcADwJ79U2JZmZWp54OFp8FvAOYBXwauAb4OLBHREzsaeaSdpV0n6R5ko7opt3HJIWkzqWo3czM\nekFPu4beFhHvBpB0OvAXYExEvNjTjCUNBU4FPgwsAG6VNDUi5ja1Ww34PHDzMtRvZmbLqactglca\nAxHxKrCgnRDItgHmRcSDEfEy6bqDVlsR3wROBNqdr5mZ9aKegmAzSU/n2zPApo1hSU/3MO0oYH7l\n/oI87m8kbQmsFxGXdTcjSQdJmilp5qJFi3pYrJmZLY2efqqyth+olzQE+C4wuae2ETEFmALQ2dkZ\nPTQ3M7OlsKxXFrdjIbBe5f7oPK5hNeBdwLWS/ghsC0z1AWMzs75VZxDcCoyTtIGk4cDewNTGgxHx\nVESMjIixETEWmAFMiIiZNdZkZmZNaguCiFgMHAJMB+4BLoyIOZKOkTShruWamdnSabeLiWUSEdOA\naU3jjuqi7U511mJmZq3VuWvIzMwGAAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZ\nFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARm\nZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeB\nmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhag0CSbtKuk/SPElHtHj8MElzJd0t6SpJ69dZj5mZvVFt\nQSBpKHAqsBswHpgkaXxTszuAzojYFLgY+M+66jEzs9bq3CLYBpgXEQ9GxMvA+cDEaoOIuCYins93\nZwCja6zHzMxaqDMIRgHzK/cX5HFdOQD4TY31mJlZC8P6uwAASfsAncCOXTx+EHAQwJgxY/qwMjOz\nwa/OLYKFwHqV+6PzuNeR9CHg34EJEfFSqxlFxJSI6IyIzo6OjlqKNTMrVZ1BcCswTtIGkoYDewNT\nqw0kbQGcRgqBR2usxczMulBbEETEYuAQYDpwD3BhRMyRdIykCbnZScAI4CJJd0qa2sXszMysJrUe\nI4iIacC0pnFHVYY/VOfyzcysZ76y2MyscA4CM7PCOQjMzArnIDAzK5yDwMyscA4CM7PCOQjMzArn\nIDAzK5yDwMyscA4CM7PCOQjMzArnIDAzK5yDwMyscA4CM7PCOQjMzArnIDAzK5yDwMyscA4CM7PC\nOQjMzArnIDAzK5yDwMyscA4CM7PCOQjMzArnIDAzK5yDwMyscA4CM7PCOQjMzArnIDAzK5yDwMys\ncA4CM7PCOQjMzArnIDAzK5yDwMyscA4CM7PCOQjMzApXaxBI2lXSfZLmSTqixeMrS7ogP36zpLF1\n1mNmZm9UWxBIGgqcCuwGjAcmSRrf1OwA4K8RsRHwPeDEuuoxM7PW6twi2AaYFxEPRsTLwPnAxKY2\nE4Ff5OGLgZ0lqcaazMysybAa5z0KmF+5vwB4T1dtImKxpKeAvwceqzaSdBBwUL77rKT7aql4xTOS\nptdiReX4BgbQ+wV+z7KS3rP1u3qgziDoNRExBZjS33X0NUkzI6Kzv+uw9vj9Gnj8niV17hpaCKxX\nuT86j2vZRtIwYA3g8RprMjOzJnUGwa3AOEkbSBoO7A1MbWozFfhUHt4TuDoiosaazMysSW27hvI+\n/0OA6cBQ4GcRMUfSMcDMiJgK/BQ4S9I84AlSWNgSxe0OG+D8fg08fs8AeQXczKxsvrLYzKxwDgIz\ns8I5CNogKSR9p3L/S5KO7mGaCa261ViGZU+WtEjSnZLmSLpY0puXd74lkvRsi3EHS9q3D5a9v6RZ\nku6WNFvSREmfknReU7uR+f1eWdJKkk6Q9L+Sbpd0k6Td6q51WbV6fZdhHutKuribx9eU9C/ttm8x\n/RmS/pD/n+6StPPy1rysJJ3eoreFfuFjBG2Q9CLwF2DriHhM0peAERFxdB8sezLQGRGH5PvnAldE\nxM/rXvZgI+nZiBjRx8sU6RTpa4AtI+IpSSOADtKp0g8CYyLi+dz+YGCbiNhf0gnAOsBBEfGSpLcA\nO0bEhX35HNrVF69v7o/s0oh41zJOf0ae/mJJHwCmRMS4XqhrWEQsXt759BdvEbRnMensgn9rfkDS\nR3OHeXdIujL/szbW5E+RtIakhyQNyeNXlTQ/r+1tKOm3km6TdL2kd3ZXRL7WYlXgr10tW9KQvAbZ\nkdsMyZ36deTbf0u6Nd+2z212zGtId+Z5rdabL96KTNLROdiRdK2kEyXdIul+Se/L44dKOim/ZndL\n+kweP0LSVXltfZakiXn8WKXOFs8EZgMbAM8AzwJExLMR8YeIeBq4DvhopaS9gfPyVt+BwKER8VKe\n7pEVNQS6kl+Lq/PrdpWkMXn8hpJm5NftW42tidx+dh7eJL8Xd+bpxwEnABvmcSc1tR8q6dt5i+tu\nSYf2UN5NpN4NGrVuJem6/P84XdI6efzWeX6NZTaWN1nSVElXA1flcYdXPif/kcetKumyvAUyW9In\n8vhrJXXm4Un5tZgt6cRKTc9KOjZPO6Px/dLrIsK3Hm6kf+DVgT+SLnr7EnB0fuzvWLJl9WngO3l4\nMnBKHv418IE8/Ang9Dx8FTAuD7+HdB1F87InA4uAO4FHgOuBoT0s+xvAF/LwLsB/5+FzgR3y8Bjg\nnjx8CbB9Hh4BDOvv17yu97HFuKOBL+Xhayuv4e7AlXn4IOBreXhlYCbpy30YsHoePxKYBwgYC7wG\nbJsfG0o6jfpPwM+Bj1aWvyfwyzy8LvDn3H5T4I7+fs164fW9BPhUHt4f+FUevhSYlIcPbkybX7vZ\nefgHwCfz8HDgTdXHW7T/LKnPsmH5/lot6jkD2DMP7wGcm4dXAn4PdOT7nyCd8g4pzLfLwydUljeZ\n1HXOWvn+LqQVRpFWsi8F3g98DPhJpYY1Kp+3zvy+/4m0lTgMuBrYI7eJxucF+M/G57C3b94iaFOk\ntbczgc81PTQamC5pFnA4sEmLyS8gfbAgrfFdoLR74L3ARZLuBE4j7QZo5YKI2Bx4K9BYTnfL/hnQ\n2O+9P+nLB+BDwCl5eVOB1XMdNwLflfQ5YM0YwJu4veB/8t/bSF8ykP7B982v282k/rDGkf7hj5N0\nN3Alae2yscb2UETMAIiIV4FdSV/69wPf05JjTJcB20taHdiLFNqv1vbs+t52pBUQgLOAHSrjL8rD\n5zZPlN0EfFXSV4D1I+KFHpb1IeC0xuc3Ip7oot1Jku7Py22sfb8DeBdwRX6fvwaMlrQmsFpE3NRF\nrVdUlrNLvt0B3A68k/Q5mQV8OG9tvi8inmqax9bAtRGxKNd+DilAAF4mBQq8/jPZqxwES+e/SF1n\nr1oZ9wPSmv+7gc8Aq7SYbiqwq6S1gK1IiT8EeDIiNq/cNu5u4ZFWCy5hyYek5bIjYj7wiKQPknqB\n/U1uP4S0ltpY3qhIuylOIG1RvAm4saddVIPcS/nvqyy54FKkXTSN122DiLgc+CRpLW6rHNSPsOT9\nf64600huiYjjSSsDH8vjXwB+C/xjHt84eDwPGJMDokgRcS4wAXgBmJY/z73h8Ih4O/AV0koTpPd4\nTuU9fndE7NLGvKrvs4DjK/PYKCJ+GhH3A1uSAuFbko5ailpfyf/38PrPZK9yECyFnPwXksKgYQ2W\n9KH0qTdMlKZ7ltTlxvdJB6pezVsYf5D0cUgHFSVt1kYZOwAPtLHs04GzgYsqa5iXA3/bbypp8/x3\nw4iYFREn5jpLDoJWpgOflbQSgKS3S1qV9Po/GhGvKB14bNm7o9KZLVtWRm0OPFS5fx5wGGlr4iaA\nSAePfwp8X6mLFpSO8Xy8d59a7X7Pkh4DPknatQkwgxyGdNGjgKS3AQ9GxMmk3aubko61dHUM6wrg\nM0rH0sgrXt05BRgi6SPAfUCHpO3ytCtJ2iQingSekdToObm73g+mA/vnrWwkjZK0tqR1gecj4mzg\nJFIoVN0C7Kh0xthQYBLp2FGfcRAsve+Q9gc3HE3avXMb3XdnewGwT/7b8EngAEl3AXN44+81NHyi\nccAM2AL4ZhvLnkra3189u+hzQGc+kDWXtG8W4AuNA2zAKyzZghhs3ixpQeV2WJvTnQ7MBW7PBwpP\nI62ZnUN6PWeRdsXd28X0KwHflnRv3u3wCeDzlcevIO0nvqCy9gdp98QiYG5e7qXA023W3B9avb6H\nAvvlz9Y/s+R5fwE4LI/fCGjeXQJpV9ns/Jq9CzgzIh4nbbXOlnRSU/vTSfva787/U//UXbH5tf4W\n8OVIv5myJ3BinvZO0q5bSCt+P8l1rNpFreStxHOBm/Jn4mJSaL0buCVP/428zOp0fwGOIJ1Zdhdw\nW0T8urvae5tPHx2k8tkI34uI9/V3LWbNlM6KeiEiQtLepAPHXa0I9StJI/JWPUrXBq0TEZ/vYbIB\nZUD8HoEtnfxh/Sxpi8NsRbQV6cQFAU+STmpYUf0/SUeSvi8fIp0tNKh4i8DMrHA+RmBmVjgHgZlZ\n4RwEZmaFcxDYgCHp77WkT6SHJS2s3B/e5jx+LukdPbT5V0kr3IF2SWdL2mN525g181lDNmDkc8gb\nF8EdTeqf5tvVNvksFEXEa13MY782lnPq8ldrNnB4i8AGPEkbSZor6RzShXnrSJoiaabSbzgcVWl7\ng6TNJQ2T9KRSf/93KfX1v3Zu8y1JX6i0P0GpF8z7JL03j19VqSfXuUq/ETGzcaV2U20LJB2Xl3Gr\npC0lXS7pAUkH5jZDJH03XyQ1S9KelfE/zBeiXUHlQkalHjGvU+op8zdq0SulUk+Zc/MFhCc2P27W\n4CCwweKdpAvoxkfEQuCIiOgENiN1+NXqB0DWAK6LiM1IXTt0dS67ImIbUsd+jVA5FHg4IsaTrvTe\nopva/pCXMYPUbcQ/kq5abVwh/nFg40atpE7p1iZd6boBMB7YL0+DpJVJ3ZV8LCK2InUl8k0qcjDs\nDmwSEZsCx3dTnxXOu4ZssHggImZW7k+SdADpM74u6ct0btM0L0REozuN24CursJu1SPpDuSeKyPi\nLklzuqltav47i9RF8nPAc5JeU+qXZgfgvNwn1MOSbiB1T/z+PP41YIGka/N8Nib1NHtl2hPGUFJ3\nyFVPkLrC/omky1jSg6XZGzgIbLD4Wy+QSj9g8nnSL309KelsWvcK+3JluLueHVv1SLo0GtO/Vhlu\n3F+W+Qm4u7vuQ3JHeJ2kLYyPk640b6c3TSuQdw3ZYLQ6qZfKp5V+ZeojNSzjRlKnaEh6N2mLY1ld\nD+ydjwm8Bdie9OM3vyN1ODhE0ihgx9x+LjBK0jZ5+cMlve53MJR+ZW71iLiU9Mt63e26ssJ5i8AG\no9tJX5b3kvqGubGGZfwAOFOpF9fGrWWvlG24GNgWuJv0i1SHRcSjSj/K/oE87z+xpIvql/IB5ZOV\nfq9gKKlX3OruqTWA/8nHE4aQurk2a8l9DZktA6U+74dFxIt5V9TlpJ8dLfnX3WyA8haB2bIZAVyV\nA0HAZxwCNlB5i8DMrHA+WGxmVjgHgZlZ4RwEZmaFcxCYmRXOQWBmVrj/A0NoTwIzx1JRAAAAAElF\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EPijoWnYu2Q",
        "colab_type": "text"
      },
      "source": [
        "### 3. Export Kaggle test result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXK6GP-LaVP0",
        "colab_type": "text"
      },
      "source": [
        "### 1. Compute predictions using Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZRelOBeZHMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kaggle_pred = lr.predict(actual_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRlrwMU0acmY",
        "colab_type": "text"
      },
      "source": [
        "### 2. Format and Export the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aUsX6rhYyQd",
        "colab_type": "code",
        "outputId": "108fb1c0-0e73-4062-a826-0f5307a44e84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "result_df = pd.DataFrame()\n",
        "result_df[\"id\"] = list(range(len(kaggle_pred)))\n",
        "result_df[\"sentiment\"] = kaggle_pred\n",
        "result_df[\"sentiment\"] = result_df[\"sentiment\"].astype(\"category\")\n",
        "result_df[\"sentiment\"].cat.categories = [\"negative\", \"positive\"]\n",
        "result_df.to_csv(\"kaggle_submission.csv\", index=False)\n",
        "result_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>9995</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>9996</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>9997</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>9998</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>9999</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows  2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id sentiment\n",
              "0        0  positive\n",
              "1        1  positive\n",
              "2        2  positive\n",
              "3        3  negative\n",
              "4        4  positive\n",
              "...    ...       ...\n",
              "9995  9995  positive\n",
              "9996  9996  positive\n",
              "9997  9997  negative\n",
              "9998  9998  negative\n",
              "9999  9999  negative\n",
              "\n",
              "[10000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvhsPQgXIKpG",
        "colab_type": "text"
      },
      "source": [
        "## Extra: GloVe: Global Vectors for Word Representation\n",
        "\n",
        "For this extra section, we describe our attempts to use pretrained Glove word embedding to vectorize the sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmpRuBJVJkGY",
        "colab_type": "text"
      },
      "source": [
        "[Download the trained word embedding here](http://nlp.stanford.edu/data/glove.twitter.27B.zip)\n",
        "\n",
        "Extract and upload the 200 dimensions word embedding to colab environment.\n",
        "\n",
        "Using this model, we have each word to map with a 200 dimensional vector, which can be used to create the feature matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-icnH1-Lcuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.preprocessing import normalize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR15JvNOHrT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load embedding\n",
        "def load(filename):\n",
        "\twith open(filename,'r') as f:\n",
        "\t  lines = f.readlines()\n",
        "\n",
        "\tembedding = dict()\n",
        "\tfor line in lines:\n",
        "\t\tfeatures = line.split()\n",
        "\t\tembedding[features[0]] = np.asarray(features[1:], dtype='float32')\n",
        "\treturn embedding\n",
        "\n",
        "embedding = load('glove.twitter.27B.200d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os8obbNbL4BB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize all words in the train and test data\n",
        "all_reviews=train_df['review'].append(test_df['review'])\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY23sgVKMjIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = list(tokenizer.word_index.keys())\n",
        "vocab_filtered = [word for word in vocab if len(word) > 2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn-V98mJM3Un",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vectorize all sentences using the pretrained models\n",
        "num_features = 200\n",
        "def vectorize(sentences, num_features):\n",
        "  mat = np.zeros((len(sentences), num_features))\n",
        "  for i, sentence in enumerate(sentences):\n",
        "    vectors = np.array([embedding[word] for word in sentence.split(\" \") if word in embedding])\n",
        "    if len(vectors) != 0:\n",
        "      mat[i] = np.sum(vectors, axis=0)\n",
        "  return mat\n",
        "\n",
        "# Each sentence is the normalized sum of the pretrained global vectors of all words\n",
        "X_train = normalize(vectorize(train_df['review'], num_features))\n",
        "X_test = normalize(vectorize(test_df['review'], num_features))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WEsOCnFNyYt",
        "colab_type": "code",
        "outputId": "be845dcc-a5bc-47e9-e259-017f8df3b75b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "# Test with logistic regression\n",
        "lr = LogisticRegression(max_iter=3000, C=0.05)\n",
        "x_train_df, x_test_df, y_train_df, y_test_df = train_test_split(X_train, train_df.sentiment, test_size=0.2, random_state=111)\n",
        "\n",
        "train_data = x_train_df, y_train_df\n",
        "test_data = x_test_df, y_test_df\n",
        "\n",
        "lr_y_pred_glove, lr_runtime_glove = test_model(lr, train_data, test_data)\n",
        "\n",
        "lr_accu_glove = calculate_accuracy(y_test, lr_y_pred_glove)\n",
        "test_results.append({'accuracy': lr_accu, 'runtime': lr_runtime_glove})"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training score: [0.74166667 0.75791667 0.74666667 0.73666667 0.73166667]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.74      0.75      0.75      2991\n",
            "    negative       0.75      0.74      0.74      3009\n",
            "\n",
            "    accuracy                           0.75      6000\n",
            "   macro avg       0.75      0.75      0.75      6000\n",
            "weighted avg       0.75      0.75      0.75      6000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9dohhAXRTDS",
        "colab_type": "text"
      },
      "source": [
        "### Conclusion:\n",
        "\n",
        "The attempt to use pre-trained GloVe word embedding seems to not have the good result . One possible reason is that the model we used is too general, and we belive we could achieve a better result if we can instead train the glove word embedding using our own review vocabulary."
      ]
    }
  ]
}